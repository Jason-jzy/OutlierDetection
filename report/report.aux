\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{review}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{scikit-learn}
\citation{friedman2001elements}
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning Models}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Logistic Regression Classifier}{2}{subsection.2.1}}
\newlabel{logistic}{{3}{2}{Logistic Regression Classifier}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gaussian Naive Bayes Classifier}{2}{subsection.2.2}}
\newlabel{GNB}{{6}{2}{Gaussian Naive Bayes Classifier}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Decision Tree Classifier}{3}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Weighted Decision Tree Classifier}{3}{subsubsection.2.3.1}}
\newlabel{weightdtc}{{2.3.1}{3}{Weighted Decision Tree Classifier}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Support Vector Classifier}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Discriminant Analysis Classifier}{4}{subsection.2.5}}
\newlabel{DA}{{2.5}{4}{Discriminant Analysis Classifier}{subsection.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Linear Discriminant Analysis Classifier}{4}{subsubsection.2.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Quadratic Discriminant Analysis Classifier}{4}{subsubsection.2.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}K-Nearest Neighbor Classifier}{4}{subsection.2.6}}
\citation{chen1603xgboost}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Boosting Methods}{5}{subsection.2.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Adaboost}{5}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}XGBoost}{5}{subsubsection.2.7.2}}
\newlabel{xgboost}{{24}{5}{XGBoost}{equation.2.24}{}}
\citation{robustcov}
\citation{isolationforest}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Other Ensemble Methods}{6}{subsection.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1}Voting}{6}{subsubsection.2.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2}Bagging}{6}{subsubsection.2.8.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.3}Random Forest}{6}{subsubsection.2.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Outlier/Novelty Detection Methods}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Robust Covariance Estimation}{6}{subsection.3.1}}
\newlabel{mahadist}{{31}{6}{Robust Covariance Estimation}{equation.3.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Isolation Forest}{6}{subsection.3.2}}
\citation{lof}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Local Outlier Factor}{7}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Our Idea: Clustering-Based Modelling Methods}{7}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Bayesian Decision Theory}{7}{subsection.4.1}}
\newlabel{optbayes}{{4.1}{7}{}{thm.4.1}{}}
\newlabel{optbayescrit}{{37}{7}{}{equation.4.37}{}}
\newlabel{param}{{40}{8}{Bayesian Decision Theory}{equation.4.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}GMDA: Gaussian Mixture Discriminant Analysis}{8}{subsection.4.2}}
\newlabel{GMDA}{{1}{8}{GMDA: Gaussian Mixture Discriminant Analysis}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Gaussian Mixture Discriminant Analysis}}{8}{algocf.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Relation and Extension with Other Methods}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Relation with Naive Bayes}{8}{subsubsection.4.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Relation with LDA and QDA}{9}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Relation with Robust Covariance Estimator(RCE)}{9}{subsubsection.4.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Relation with Decision Tree}{9}{subsubsection.4.3.4}}
\newlabel{GMDA-DTC}{{2}{9}{Relation with Decision Tree}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces GMDA with Decision Tree Classifier}}{9}{algocf.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Relation with Ensemble Methods}{9}{subsubsection.4.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Case Study: Credit Card Fraud Detection}{9}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data Description}{9}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Data Preprocessing}{10}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Implementation Details}{10}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Supervised Learning Methods}{10}{subsubsection.5.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Outlier Detection Methods}{10}{subsubsection.5.3.2}}
\bibdata{reference}
\bibcite{lof}{1}
\bibcite{review}{2}
\bibcite{chen1603xgboost}{3}
\bibcite{friedman2001elements}{4}
\bibcite{isolationforest}{5}
\bibcite{scikit-learn}{6}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}GMDA and Related Methods}{11}{subsubsection.5.3.3}}
\newlabel{feature}{{43}{11}{GMDA and Related Methods}{equation.5.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Comparison between Different Methods}{11}{subsection.5.4}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{11}{section.6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison between different methods. Each row represent one kind of algorithm. The first eleven rows are about supervised learning method, and the next six rows are about Gaussian mixture model methods, and the last three rows are about outlier detection methods. We have presented the recall (R), precision (P) and F-score (F) on the training, validation and test sets above, where the definition of recall, precision and F-score are given in the first section. The last two columns are about time costs on training process and prediction process. (on test set)}}{12}{table.1}}
\newlabel{comparison}{{1}{12}{Comparison between different methods. Each row represent one kind of algorithm. The first eleven rows are about supervised learning method, and the next six rows are about Gaussian mixture model methods, and the last three rows are about outlier detection methods. We have presented the recall (R), precision (P) and F-score (F) on the training, validation and test sets above, where the definition of recall, precision and F-score are given in the first section. The last two columns are about time costs on training process and prediction process. (on test set)}{table.1}{}}
\bibcite{robustcov}{7}
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorem \ref  {optbayes}}{13}{appendix.A}}
\newlabel{LastPage}{{}{13}{}{page.13}{}}
\xdef\lastpage@lastpage{13}
\xdef\lastpage@lastpageHy{13}
