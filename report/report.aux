\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{scikit-learn}
\citation{friedman2001elements}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning Models}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Logistic Regression Classifier}{1}{subsection.2.1}}
\newlabel{logistic}{{1}{1}{Logistic Regression Classifier}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gaussian Naive Bayes Classifier}{2}{subsection.2.2}}
\newlabel{GNB}{{4}{2}{Gaussian Naive Bayes Classifier}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Decision Tree Classifier}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Weighted Decision Tree}{2}{subsubsection.2.3.1}}
\newlabel{weightdtc}{{2.3.1}{2}{Weighted Decision Tree}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Support Vector Classifier}{2}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Discriminant Analysis Classifier}{3}{subsection.2.5}}
\newlabel{DA}{{2.5}{3}{Discriminant Analysis Classifier}{subsection.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Linear Discriminant Analysis Classifier}{3}{subsubsection.2.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Quadratic Discriminant Analysis Classifier}{3}{subsubsection.2.5.2}}
\citation{chen1603xgboost}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}K-Nearest Neighbor Classifier}{4}{subsection.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Boosting Methods}{4}{subsection.2.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Adaboost}{4}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}XGboost}{4}{subsubsection.2.7.2}}
\newlabel{xgboost}{{22}{4}{XGboost}{equation.2.22}{}}
\citation{robustcov}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Random Forest / Bagging / Voting Methods}{5}{subsection.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1}Voting Method}{5}{subsubsection.2.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2}Bagging Method}{5}{subsubsection.2.8.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.3}Random Forest Method}{5}{subsubsection.2.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Outlier/Novelty Detection Methods}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Robust Covariance Estimator}{5}{subsection.3.1}}
\newlabel{mahadist}{{29}{5}{Robust Covariance Estimator}{equation.3.29}{}}
\citation{isolationforest}
\citation{lof}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Isolation Forest}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Local Outlier Factor}{6}{subsection.3.3}}
\@writefile{toc}{\contentsline {paragraph}{}{6}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Our Idea: Clustering-Based Modelling Methods}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Bayesian Decision Theory}{6}{subsection.4.1}}
\newlabel{optbayes}{{4.1}{7}{}{thm.4.1}{}}
\newlabel{optbayescrit}{{35}{7}{}{equation.4.35}{}}
\newlabel{param}{{38}{7}{Bayesian Decision Theory}{equation.4.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}GMDA: Gaussian Mixture Discriminant Analysis}{7}{subsection.4.2}}
\newlabel{GMDA}{{1}{7}{GMDA: Gaussian Mixture Discriminant Analysis}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Gaussian Mixture Discriminant Analysis}}{7}{algocf.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Relation and Extension with Other Methods}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Relation with Naive Bayes}{8}{subsubsection.4.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Relation with LDA and QDA}{8}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Relation with Robust Covariance Estimator(RCE)}{8}{subsubsection.4.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Relation with Decision Tree}{8}{subsubsection.4.3.4}}
\newlabel{GMDA-DTC}{{2}{8}{Relation with Decision Tree}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces GMDA with Decision Tree Classifier}}{8}{algocf.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiment}{8}{section.5}}
\bibdata{reference}
\bibcite{lof}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Supervised Learning Methods}{9}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Outlier Detection Methods}{9}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}GMDA and Related Methods}{9}{subsection.5.3}}
\newlabel{feature}{{41}{9}{GMDA and Related Methods}{equation.5.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Comparison between Different Methods}{9}{subsection.5.4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison between different methods. Each row represent one kind of algorithm. The first eleven rows are about supervised learning method, and the next six rows are about Gaussian mixture model methods, and the last three rows are about outlier detection methods. We have presented the recall (R), precision (P) and F-score (F) on the training, validation and test sets above, where the definition of recall, precision and F-score are given in the first section. The last two columns are about time costs on training process and prediction process. (on test set)}}{10}{table.1}}
\newlabel{comparison}{{1}{10}{Comparison between different methods. Each row represent one kind of algorithm. The first eleven rows are about supervised learning method, and the next six rows are about Gaussian mixture model methods, and the last three rows are about outlier detection methods. We have presented the recall (R), precision (P) and F-score (F) on the training, validation and test sets above, where the definition of recall, precision and F-score are given in the first section. The last two columns are about time costs on training process and prediction process. (on test set)}{table.1}{}}
\bibcite{chen1603xgboost}{2}
\bibcite{friedman2001elements}{3}
\bibcite{isolationforest}{4}
\bibcite{scikit-learn}{5}
\bibcite{robustcov}{6}
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{11}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorem \ref  {optbayes}}{12}{appendix.A}}
\newlabel{LastPage}{{}{12}{}{page.12}{}}
\xdef\lastpage@lastpage{12}
\xdef\lastpage@lastpageHy{12}
