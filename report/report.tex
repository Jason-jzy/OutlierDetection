\documentclass[english]{article}
\RequirePackage{fancyhdr}
\usepackage{times}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{lastpage}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{indentfirst}
\usepackage{tabu}
\usepackage{multirow}
\newenvironment{eqt}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newtheorem{defn}{Definition}[section]
\newtheorem{thm}{Theorem}[section]


\fancypagestyle{plain}{
	\fancyhf{}\renewcommand*{\headrulewidth}{0.75bp}
	\fancyhead[R]{\normalfont{Page \thepage{} of \pageref*{LastPage}}}
	\fancyhead[L]{\normalfont{\cuniversity}}
	\fancyhead[C]{\normalfont{\titlemark}}
}
\pagestyle{plain}

\geometry{
	a4paper, includefoot, hmargin = 2.6cm, top = 2.7cm, bottom = 2cm,
	headheight = 1.5cm, headsep = 0.5cm, footskip = 0.75cm
}

\DeclareRobustCommand{\authorthing}{
\begin{tabular}{cc}
\large{Zeyu Jia}\thanks{The authors are arranged lexicographically.} & \large{Feng Zhu}\footnotemark[1]\\
1600010603 & 1600010643
\end{tabular}\\
\\
School of Mathematical Science, Peking University
}

\newcommand{\cuniversity}{Peking University}
\newcommand{\titlemark}{A Brief Survey on Outlier Classification}
\newcommand{\Input}{\textbf{Input}}
\newcommand{\Output}{\textbf{Output}}
\author{\authorthing}
\title{\textbf{\titlemark}}

\begin{document}
\maketitle

\abstract{In this report, we will focus on solving outlier detection problems. We have proposed several algorithms based on Gaussian mixture model to solve such problems. These algorithms, which combine techniques of supervised learning and unsupervised learning, perform quite well on these tasks. Furthermore, we have also discussed and implemented several supervised learning algorithms and existed outlier detection methods. We make comparison about performance of these algorithms on an outlier detection dataset among these algorithms as well.}

\section{Introduction}
\par Anomaly detection (or outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, anomalous data can be connected to some kind of problem or rare event such as, e.g., bank fraud, medical problems, structural defects, etc. This connection makes it very interesting to be able to pick out which data point can be considered anomalies, as identifying these events are typically essential from a business perspective. There have been plentiful literatures on machine learning methods for outlier detection, and they generally focus on unsupervised or semi-supervised ones. For a review of them, readers could refer to \cite{review}.
\par However, unlike labelling of pictures or videos, it is not that difficult to label data points in anomaly detection setting. For example, in financial fraud detection, it's easy to obtain whether a deal is fraud or not after it is done. We observe that labeled data for outlier detection is always available for training and testing numerous algorithms. Why do we have to apply unsupervised or semi-supervised learning methods given that the labels are complete? Thus, in this paper, we will mainly focus on ``Outlier Classification", where labels are all presented.
\indent Outlier Classification gives rise to other challenging problems. 
\begin{enumerate}
\item The dataset is highly imbalanced. The ratio between normal data and anomaly ones may be very large, sometimes exceeding 500:1. Such imbalanced proportion between each class will let traditional classifiers tend to classify instances to normal class much easily, and will certainly bring ineffective outcome since classifying anomaly instances into normal will not affect the accuracy very much. Since what we want is to identify outliers accurately, this phenomenon will seriously damage the effect of traditional classifiers.
\item The number of anomaly data is small, even if the size of the whole data set is large. The problem become especially serious when we split the dataset into several parts, because it imposes high volatility on evaluation with regard to anomaly data.
\end{enumerate} 
\par To deal with the problems above, we need to clarify our evaluation metric before going further. We will use \textit{recall}(R) and \textit{precision}(P), which is defined as
\begin{eqt}
\textit{recall}\text{(R)} & = \frac{\text{TP}}{\text{TP}+\text{FN}},\\ 
\textit{precision}\text{(P)} & = \frac{\text{TP}}{\text{TP}+\text{FP}},
\end{eqt}
where TP, FP, FN are defined as the number of instances in the following table
\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
			& True Anomaly & True Normal\\
		\hline
			Predicted Anomaly & TP & FP\\
		\hline
			Predicted Normal & FN & TN\\
		\hline
	\end{tabular}
\end{table}
\par To integrate the two metric, we also define our \textit{F-score}. In practice, it is always more important to detect all true anomalies as much as possible, which means that we should put more weight on \textit{recall} than \textit{precision}. Our \textit{F-score}(F) is defined as
\begin{eqt}
\textit{F-score}\text{(F)} = \frac{2.5\times \text R\times \text P}{1.5\times \text P + \text R}.
\end{eqt}
\par Our paper is organized as follows. Section 2 will briefly introduce several supervised learning techniques. Among them, we pay special attention to (weighted) Decision Tree Classifier, XGBoost, and Ensemble methods. Section 3 will briefly introduce three commonly used unsupervised learning techniques for outlier detection. In Section 4, we will introduce our algorithm, ``GMDA", which is originally based on Bayesian decision theory, to combine both supervised and unsupervised learning methods. We also explain its relation and extension with other methods. We conduct a case study of financial fraud detection in Section 5 to produce plentiful results and compare different algorithms. In Section 6, we conclude. The division of our work is in Acknowledgement.

\section{Supervised Learning Models}
\par In this section, we will discuss several commonly used supervised learning methods, and their application on outlier classification tasks. These supervised learning methods includes
\begin{itemize}
	\item Logistic Regression Classifier,
	\item Gaussian Naive Bayes Classifier,
	\item Decision Tree Classifier,
	\item Support Vector Classifier,
	\item Discriminant Analysis Classifier,
	\item K-Nearest Neighbor Classifier.
\end{itemize}
\par Furthermore, we will also introduce two useful techniques used in classification problem: boosting methods(Adaboost/XGBoost) and other ensemble methods(random forest/bagging/voting). These methods and techniques refer to \cite{scikit-learn}\cite{friedman2001elements}, and will be discussed in the following subsections respectively.

\subsection{Logistic Regression Classifier}
\par The logistic regression classifier is one of the most simple methods used in 2-class classification problem. Suppose two class are denoted by $0$ and $1$. Then we will find an proper $W$ and $b$ such that
\begin{equation}\label{logistic}
	\begin{aligned}
		&P(y=0|X=x, \beta=(W, b)) = \frac{1}{1 + \exp(Wx+b)},\\
		&P(y=1|X=x, \beta=(W, b)) = \frac{\exp(Wx+b)}{1 + \exp(Wx+b)},\\
	\end{aligned}
\end{equation}
and we will maximize the log-likelihood:
\begin{equation}
	l(\beta) = \sum_{i=1}^{N}y_{i}\log P(y=y_{i}|X=x_{i}, \beta) + (1-y_{i})\log P(y\neq y_{i}|X=x_{i}, \beta).
\end{equation}
By finding the optimal parameter $\beta$, then we can use \eqref{logistic} to make prediction.

\subsection{Gaussian Naive Bayes Classifier}
\par Gaussian naive Bayes classifier is one of Bayesian classifier using Gaussian distribution as prior. We make the assumption that
\begin{equation}
	P(X|y) = \prod_{i=1}^{m}P(x_{i}|y),
\end{equation}
where $X = (x_{1}, \cdots, x_{m})$ is the feature vector. This assumption talks about the independence of each element of feature vector. And then we can use MAP to make classification:
\begin{equation}\label{GNB}
	\hat{y} = \arg\max_{y}P(y|X) = \arg\max_{y}\frac{P(y)\prod_{i=1}^{m}P(x_{i}|y)}{P(X)} = \arg\max_{y}P(y)\prod_{i=1}^{m}P(x_{i}|y).
\end{equation}
And here we assume the likelihood given each $y$ to be Gaussian:
\begin{equation}
	P(x_{i}|y)\sim\mathcal{N}(\mu_{y}, \sigma_{y}),
\end{equation}
where parameters $\mu_{y}$ and $\sigma_{y}$ are determined by maximum likelihood estimation. After finishing this estimation, the classification process can be done according to \eqref{GNB}.
\par The underlying assumption of this method is that data in each class satisfies Gaussian distribution, and each features are independent given its label.

\subsection{Decision Tree Classifier}
\par The decision tree classifier is a little bit different to other methods. Other methods all aim to find a proper linear (or nonlinear) combination of features. The decision tree, however, will execute on features sequentially. In other words, in each step of decision tree's iteration, only one feature will be taken into consideration.
\par The training of a decision tree is often divided into two parts: splitting and pruning. In the splitting part, the decision tree will expand their leaves. That is, finding a new feature (often the one with the most reduction of the loss) and then split one of the tree leaf into two new leaves. In the pruning process, some of leaves will merged into one, if merging them can decrease the value of loss plus regularization.

\subsubsection{Weighted Decision Tree Classifier}
\label{weightdtc}
\par Weighted decision tree is a useful model in tackling cases where number of instances in each class varies a lot.
\par To balance the number of instances in each class, and to let the classifier be not prone to classify new instance into the major class, we assign a weight to each class. And we aim to let the number of instances in each class multiplied by the weight in that class does not vary a lot. Then we will train a decision tree with weights on these instances. Specifically speaking, suppose the training set is 
\begin{equation}
	\{(x_{1}, y_{1}), (x_{2}, y_{2}), \cdots, (x_{n}, y_{n})\},
\end{equation}
where $y_{i}\in Y = \{1, 2, \cdots, N\}$. If we add weight $\alpha_{i}$ to class $i$ $(1\le i\le N)$, then training the weighted decision tree with such weights is equivalent to training a decision tree with samples:
\begin{equation}
	\begin{matrix}
		(x_{1}, y_{1})\times \alpha_{y_{1}},\\
		(x_{2}, y_{2})\times \alpha_{y_{2}},\\
		\vdots\\
		(x_{n}, y_{n})\times \alpha_{y_{n}},
	\end{matrix}
\end{equation}
where we duplicate sample $(x_{i}, y_{i})$ by $\alpha_{y_{i}}$ times.
\par Adopting this technique, the inclination to classify a new instance to major classes reduces a lot.

\subsection{Support Vector Classifier}
\par The support vector classifier is also termed with support vector machine for classification. 
\par Estimating parameters of common SVM (linear SVM) can be done by solving the following optimization problem:
\begin{equation}
	\begin{aligned}
		& \min_{\beta, \beta_{0}}\frac{1}{2}\|\beta\|^{2} + C\sum_{i=1}^{n}\xi_{i}\\
		& \text{subject to }\xi_{i}\ge 0, y_{i}(x_{i}^{T}\beta + \beta_{0})\ge 1 - \xi_{i}, \quad \forall i
	\end{aligned}
\end{equation}
\par We can solve it by writing its dual problem:
\begin{equation}
	\begin{aligned}
		& \max_{\alpha_{i}}\sum_{i=1}^{n}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j},\\
		& \text{subject to } 0\le \alpha_{i}\le C, \quad\sum_{i=1}^{n}\alpha_{i}y_{i} = 0.
	\end{aligned}
\end{equation}
\par In order to handle data that cannot be linear separated, we introduce the kernel function, which elevate the feature $x_{i}$ into $h(x)$. If $\langle h(x), h(y)\rangle = K(x, y)$, where this $K(x, y)$ is called the kernel, then the dual problem becomes
\begin{equation}
	\begin{aligned}
		& \max_{\alpha_{i}}\sum_{i=1}^{n}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i}, x_{j}),\\
		& \text{subject to } 0\le \alpha_{i}\le C, \quad\sum_{i=1}^{n}\alpha_{i}y_{i} = 0,
	\end{aligned}
\end{equation}
and the prediction step can be written as
\begin{equation}
	f(x) = h(x)^{T}\beta + \beta_{0} = \sum_{i=1}^{n}\alpha_{i}y_{i}K(x, x_{i}) + \beta_{0}.
\end{equation}
We can classify the instance $x$ according to the sign of $f(x)$.
\par In our implementation, we use the RBF kernel 
\begin{equation}
	K(x, y) = \exp\left(-\frac{\|x-y\|^{2}}{2\sigma^{2}}\right)
\end{equation}
to make classification.

\subsection{Discriminant Analysis Classifier}
\label{DA}
\par The discriminant analysis classifiers we implemented include the linear discriminant analysis classifier (LDA) and quadratic discriminant classifier (QDA).
\subsubsection{Linear Discriminant Analysis Classifier}
\par In LDA setting, the underlying assumption is that data in each class satisfies Gaussian distribution. Furthermore, we also assume the covariance matrix in each class shares the same one. 
\par To use this model to classify a new instance, firstly, we need to estimate the probability of each class $\hat{\pi}_{k}$, the mean of the data distribution of each class $\hat{\mu}_{k}$, and the covariance matrix $\Sigma$:
\begin{itemize}
	\item $\hat{\pi}_{k} = N_{k} / N$, where $N_{k}$ is the number of instances in class $k$ and $N$ is the total number of instances. ($k = 1, 2$),
	\item $\hat{\mu}_{k} = \sum_{y_{i} = k}x_{i} / N_{k}$,
	\item $\hat{\Sigma} = \sum_{k=1}^{2}\sum_{y_{i} = k}(x - \hat{\mu}_{k})(x - \hat{\mu}_{k})^{T} / (N - 2)$.
\end{itemize}
Then we can use the following quantity
\begin{equation}
	\delta_{k}(x) = x^{T}\Sigma^{-1}\mu_{k} - \frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k} + \log\pi_{k}
\end{equation}
to determine the estimated class of instance $x$, according to
\begin{equation}
	\hat{y} = \arg\max_{k\in\{1, 2\}}\delta_{k}(x).
\end{equation}

\subsubsection{Quadratic Discriminant Analysis Classifier}
\par In LDA's setting, we assume the covariance matrix in each class to be the same, but sometimes this assumption do not always hold, and we need to estimate the covariance matrix for each class. This estimation is called quadratic discriminant analysis.
\par The estimating method for the covariance in class $k$ is
\begin{equation}
	\hat{\Sigma}_{k} = \frac{1}{N_{k} - 1}\sum_{y_{i} = k}(x_{i} - \hat{\mu}_{k})(x_{i} - \hat{\mu}_{k})^{T},
\end{equation}
where $N_{k}$ is the total number of training data classified into class $k$, and $\mu_{k}$ is the estimated mean according to the formula in LDA.
\par In QDA setting, we can use the following quantity
\begin{equation}
	\delta_{k}(x) = -\frac{1}{2}\log|\hat{\Sigma}_{k}| - \frac{1}{2}(x - \mu_{k})^{T}\Sigma_{k}^{-1}(x - \mu_{k}) + \log\pi_{k}
\end{equation}
to determine the estimated class of instance $x$, according to
\begin{equation}
	\hat{y} = \arg\max_{k\in\{1, 2\}}\delta_{k}(x).
\end{equation}
\par Note that the QDA classifier is similar to the naive Bayes classifier, only lack of the assumption that each feature is independent given the class label. Hence if we assume the covariance matrices $\hat{\Sigma}_{k}$ to be diagonal, then we will obtain the same results to naive Bayes classifier.

\subsection{K-Nearest Neighbor Classifier}
\par The K-Nearest Neighbor Classifier (KNN Classifier) is the simplest kernel smoothing classifier. Its idea is to label a new instance according to the majority among the labels of its nearest $k$ instances.
\par Specifically, suppose the training set is denoted by $\{(x_{i}, y_{i})\}_{1\le i\le n}$ ($y_{i}\in\{0, 1\}$), and we have a new instance $x$. If the $k$ nearest neighbors to $x$ among $x_{i}$ are $x_{1}\cdots, x_{k}$, then we will label it according to
\begin{equation}
	\hat{y} = \begin{cases}
		0 & \quad \text{if }\sum_{i=1}^{k}y_{i}\le k / 2,\\
		1 & \quad \text{if }\sum_{i=1}^{k}y_{i}> k / 2.\\
	\end{cases}
\end{equation}
\par The training process of the KNN method takes very short time. However, in the predicting process, for every new instance, we need to search over the training set to find $k$ nearest neighbor to the new instance, which will be quite time costly if the training set is extremely large.

\subsection{Boosting Methods}
\par We will discuss two boosting methods we have implemented in this subsection:
\begin{itemize}
	\item Adaboost
	\item XGboost
\end{itemize}

\subsubsection{Adaboost}
\par Adaboost method is one of the most commonly used ensemble method aiming to improve the accuracy on the training set.
\par The adaboost method can be viewed as a method which improves the performance on the training set by training a model with larger weight on data misclassified and smaller weight on data classified correctly.
\par Specifically, suppose we have $m-1$ classifier $G_{1}, \cdots, G_{m-1}$ with weight $\beta_{1}, \cdots, \beta_{m-1}$. And we have function $f_{i}$ with $f_{i}(x) = f_{i-1}(x) + \beta_{i}G_{m}(x)$. Then we will train the $m$-th classifier $G_{m}$ and its weight $\beta_{m}$ to minimize
\begin{equation}
	\beta_{m}, G_{m} = \arg\min_{\beta, G}\sum_{i=1}^{n}\exp(-y_{i}(f(x_{i}) + \beta G(x_{i}))),
\end{equation}
and then we let
\begin{equation}
	f_{m}(x) = f_{m-1}(x) + \beta_{m}G_{m}(x).
\end{equation}

\subsubsection{XGBoost}
\par This part refers to the paper \cite{chen1603xgboost}. XGBoost is the abbreviation of ``extreme gradient boosting'' method. This is also a method using the boosting tree technique. But instead of using gradient boosting, the main idea of XGBoost is to use both the first and the second order derivative to approximate the loss function. In order to understand the boosting tree technique better, we refine the definition of tree as follows. We assign each leaf with a score, describing how good the current leaf is. And when we use the decision tree for some tasks, we will go through the tree from the the root to leaves, and finally get the score corresponding to that leaf.
\par To describe the training process, we first define the loss function of the model:
\begin{equation}
	\mathcal{L} = \sum_{i=1}^{n}l(y_{i}, \hat{y}_{i}) + \sum_{k=1}^{K}\Omega(f_{k}),
\end{equation}
where $K$ is the number of trees in this model, $\Omega$ is the regularization function, and $\hat{y}_{i}$ is defined as
\begin{equation}
	\hat{y}_{i} = \sum_{k=1}^{K}f_{k}(x_{i}).
\end{equation}
\par Suppose the tree we aim to train at $t$-th iteration is $f_{t}$, where $f_{t}(x)$ denotes the score we will obtain by putting the instance $x$ into tree $f_{t}$. Then if we want to boost the model using this tree $f_{t}$, the loss function is 
\begin{equation}
	\mathcal{L}^{(t)} = \sum_{i=1}^{n}l(y_{i}, \hat{y}_{i}^{(t-1)} + f_{t}(x_{i})) + \Omega(f_{t}) + const,
\end{equation}
where $\hat{y}_{i}^{(t-1)}$ is the predicted value on $x_{i}$ according to the first $t-1$ trees. Next, in order to minimize this loss, we find the Taylor expansion:
\begin{equation}\label{xgboost}
	\mathcal{L}^{(t)}\approx\sum_{i=1}^{n}\left[l(y_{i}, \hat{y}_{i}^{(t-1)}) + g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right] + \Omega(f_{t}) + const,
\end{equation}
where
\begin{equation}
	g_{i} = \partial_{\hat{y}^{(t-1)}}l(y_{i}, \hat{y}^{(t-1)}), \quad h_{i} = \partial^{2}_{\hat{y}^{(t-1)}}l(y_{i}, \hat{y}^{(t-1)}).
\end{equation}
\par Normally, we choose the function $l$ as the squared loss function
\begin{equation}
	l(y_{i}, \hat{y}_{i}) = (y_{i} - \hat{y}_{i})^{2}.
\end{equation}
Then it is easy to calculate 
\begin{equation}
	g_{i} = 2(\hat{y}^{(t-1)} - y_{i}), \quad h_{i} = 2.
\end{equation}
\par Furthermore, the regularization term $\Omega(f)$ is often chosen to be
\begin{equation}
	\Omega(f) = \gamma T + \frac{\lambda}{2}\sum_{j=1}^{T}\omega_{j}^{2},
\end{equation}
where $T$ is the number of leaves in the tree $f$, and $\omega_{j}$ is the score according to leaf $j$.
\par In order to find the most proper tree model $f_{t}$, we only need to find the optimizer of \eqref{xgboost}.
\par In comparison with gradient boosting methods, the XGBoost methods use order-2 Taylor expansion, while gradient boosting use order-1 Taylor expansion. (only consider the gradient of the loss functiion) Specifically, in gradient boosting methods, we consider the following estimation of the loss function:
\begin{equation}
	\mathcal{L}^{(t)}\approx\sum_{i=1}^{n}\left[l(y_{i}, \hat{y}_{i}^{(t-1)}) + g_{i}f_{t}(x_{i})\right] + \Omega(f_{t}) + const
\end{equation}
Hence if we let $f_{t}(x) = \alpha_{t}f_{t}^{*}(x)$, then we only need to fit the estimator $f_{t}^{*}$ to the negative gradient direction, and in such ways we can obtain the largest descent if we increase the parameter $\alpha_{t}$ from zero.
\par Therefore, due to the difficulty in obtaining the second order derivative, XGBoost is usually slower than gradient boosting. But the second order derivative can make XGBoost more accurate than gradient boosting, and hence in less iterations, XGBoost methods can achieve similar results to gradient boosting methods.

\subsection{Other Ensemble Methods}
\par Methods discussed in this subsection are similar in that they all adopt the idea of bagging or voting to make the classifier more powerful. We will discussed voting method, bagging method and the random forest method here in this subsection.

\subsubsection{Voting}
\par Voting is a method balancing different classifiers in order to achieve a better classifier.
\par Suppose their are $n$ classifier $f_{1}, \cdots, f_{m}$. Then we assign weights $w_{1}, \cdots, w_{m}$ to each classifier. Then the idea of voting is to choose the label which is most likely been classified. That is, we choose 
\begin{equation}
	\hat{y} = \max_{y}\#\{f_{m}(x) = y, 1\le i\le m\}.
\end{equation}

\subsubsection{Bagging}
\par Bagging adopts the idea of voting method, uses one model to train several classifiers by bootstrap, and then use these classifiers to vote to get the prediction. This type of methods can reduce the variance, and is easy for parallel computing.

\subsubsection{Random Forest}
\par Based on Bagging, Random Forest goes one step further. In this method, the basic model is chosen to be decision tree, and in the bootstrap step, not only does the data apply bootstrap technique, but also the features are selected randomly.

\section{Outlier/Novelty Detection Methods}

\par
In this section, we will introduce some standard outlier detection methods. As we have mentioned in Introduction, all these methods assume loss of labels and try to empirically figure out whether a collected data point is an ``outlier" or a new data point is a ``novelty". 

\subsection{Robust Covariance Estimation}
\par
The estimator obtained from the algorithm is also called Minimum Covariance Determinant (MCD) estimator. It is, to some extent, similar to LDA. The algorithm fits only one Gaussian distribution to model the normal data in a robust way. More rigorously, it tries to find 
\begin{eqt}
\frac{n_{\textit{samples}}+n_{\textit{features}}+1}{2}
\end{eqt}
observations whose empirical covariance has the smallest determinant and yields a ``pure" subset of observations. We then could estimate so-called Mahalanobis distance between any data point $x$ and the center of fitted Gaussian distribution, i.e., 
\begin{eqt}
\label{mahadist}
d(x; \mu, \Sigma) = (x-\mu)'\Sigma^{-1}(x-\mu).
\end{eqt}
\par
If it's value is too large, then $x$ is classified as an anomaly, else a normal data point. Commonly, there is no tuning parameters. For more detailed description, readers could refer to \cite{robustcov}.


\subsection{Isolation Forest}
\par
The algorithm borrows ideas form Random Forest. It first randomly select a feature, and randomly select a splitting value between the maximum and minimum values of the selected feature. Then it recursively conduct this process to form a tree. The number of splittings to ``isolate" a sample is equivalent to the path length from the root node to the terminating node. Finally, the path length is averaged over a forest of such random trees, and embedded in an anomaly score, which is regarded as a measure of normality. The anomaly score in this algorithm is defined as 
\begin{eqt}
s(X, n) = 2^{-\frac{E[h(X)]}{c(n)}},
\end{eqt}
where $h(X)$ is the path length of observation $X$, $E[h(X)]$ is the average of $h(X)$ through the forest of trees. $c(n)$ is the average path length of unsuccessful search in a Binary Search Tree and $n$ is the number of its external nodes. If we ignore the detailed derivation of $c(n)$, we could regard it as a normalization term. More on the anomaly score and its components can be found in \cite{isolationforest}.
\par
In fact, random partitioning produces shorter paths for anomalies because anomalies have high probability of being close to the extreme values of a feature. Hence, if for a particular sample, the forest of random trees collectively produce shorter path lengths, it is highly likely to be an anomaly.
\par The tuning parameters in this algorithm are the number of trees and the proportion of outliers.

\subsection{Local Outlier Factor}
\par
The algorithm computes a score (called local outlier factor) to reflect the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.
\par
Commonly, the local density is obtained from KNN. The LOF score of an observation is equal to the ratio of the average local density of its $k$-nearest neighbors and its own local density. A normal instance is expected to have a local density similar to or higher than that of its neighbors, while abnormal data are expected to have much smaller local density.
\par
Now let us describe the algorihm more rigirously. Denote $N_k(X)$ as the set of the first $k$ nearest neighbors of data point $X$ and $d(X, Y)$ as the distance between $X$ and $Y$. Define $d_k(X)$ as the distance between $X$ and its $k$th nearest neighbor. The $k$-reachability distance between $X$ and $Y$ is defined as
\begin{eqt}
\textit{rd}_k(X, Y) = \max\{d_k(Y), d(X, Y)\}.
\end{eqt}
The $k$-local reachability of $X$ is defined as 
\begin{eqt}
\textit{lrd}_k(X) = \frac{|N_k(X)|}{\sum_{Y\in N_k(X)}\textit{rd}_k(X, Y)}.
\end{eqt}
The $k$-local outlier factor of $X$ is defined as 
\begin{eqt}
\textit{lof}_k(X) = \frac{\sum_{Y\in N_k(X)}\textit{lrd}_k(Y)}{|N_k(X)|\textit{lrd}_k(X)}.
\end{eqt}

\par If $\textit{lof}_k(X)$ is larger than a threshold, then we classify $X$ is an outlier. 
\par The tuning parameters in this algorithm are the number of neighbors $k$ and the proportion of outliers. For more detailed description of the algorithm, readers could refer to \cite{lof}.

\section{Our Idea: Clustering-Based Modelling Methods}
\par
In this part, we will explicitly describe our algorithm on outlier classification. We focus on modelling data distribution with different labels and apply a variant of Bayesian decision theory for classification. Such idea can be easily integrated with other supervised or unsupervised learning technique such as decision tree, ensemble methods, and EM algorithm.

\subsection{Bayesian Decision Theory}
\par
To begin with, we describe some general theories. We now assume the following conditions.
\begin{enumerate}
\item Suppose there are $k$ different classes $G_1, ..., G_k$. 
\item For any given data $x\in \mathbb{R}^n$, the prior probability that it belongs to $G_k$ is $p_k$ (Thus we must have $\sum_{i=1}^kp_i=1$). A criterion $D$ is a partition of $\mathbb{R}^n$, $\{D_1, ..., D_k\}$, such that we classify $x$ into $G_i$ if and only if $x\in D_i (i\in[k])$.
\item Let $L(j|i)$ be the loss when we classify an object in $G_i$ to $G_j$. Generally, when $i=j$, $L(j|i) = 0$.
\item Let $P(j|i;D)$ be the probability that we classify an object in $G_i$ into $G_j$ under the criterion $D$.
\end{enumerate}
\par
With these assumptions, we now give the following definition.

\begin{defn}
Let $g(D) := \sum_{i=1}^kp_i\sum_{j=1}^kP(j|i;D)L(j|i)$ to be the average loss under classification criterion $D$. If there exists a $D^*$ such that
\begin{eqt}
g(D^*) = \min_Dg(D)
\end{eqt}
We call $D^*$ the optimal Bayesian (classification) criterion.
\end{defn}
If we add some conditions, we will have the following theorem.
\begin{thm}
\label{optbayes}
Suppose the joint distribution density of the $i$th class $G_i$ is $f_i(X)$, then the optimal Bayesian criterion $D^*=(D_1^*, ..., D_k^*)$ is
\begin{eqt}
\label{optbayescrit}
D_i^* = \{X|h_i(X)<h_j(X), \forall j\neq i\},
\end{eqt}
where $h_j(X) = \sum_{i=1}^kp_iL(j|i)f_i(X)$, i.e., the average loss when we classify $X$ into $G_j$.
\end{thm}
\begin{proof}
For detailed proof, please see the appendix.
\end{proof}
\par
In our setting, $k=2$. Then from Theorem \ref{optbayes},
\begin{eqt}
h_1(X) = p_2f_2(X)L(1|2),\indent h_2(X) = p_1f_1(X)L(2|1)
\end{eqt}
Therefore, the optimal Bayesian solution is 
\begin{eqt}
&D_1 = \{X|W(X)>d\}, \\
&D_2 = \{X|W(X)\leqslant d\},
\end{eqt}
where
\begin{eqt}
\label{param}
W(X) = \frac{f_1(X)}{f_2(X)},\indent d = \frac{p_2L(1|2)}{p_1L(2|1)}.
\end{eqt}
Once we obtain the densities, the prior probabilities and the loss, we could make optimal Bayesian decision.

\subsection{GMDA: Gaussian Mixture Discriminant Analysis}
\par
To determine the density $f(X)$, we have numerous choices. For both fitness of generalization and speed of optimization, we choose to model $f(X)$ using Gaussian Mixture. Many kinds of data in real life can be well approximated by this model and well-performed algorithms such as EM algorithm are widely applied for optimization.
\par
For parameters in $d$(see \ref{param}), prior probabilities $p_1$ and $p_2$ could be inferred from data. However, generally, the loss $L(1|2)$ and $L(2|1)$ are determined by decision makers and we need to find a criterion to decide the ratio. Therefore, we should regard $d$ as a tuning parameter. 
\par
Based on previous analysis, our algorithm is as follows. We split the data into Training Set, Validation Set and Test Set. We first use Training Set to model the distribution of normal data ($f_1(X)$) and anomaly data ($f_2(X)$). We then use Validation Set to determine an optimal $d$. Finally, we apply the tuned $d$ to test performance on Test Set. The formal formulation is in Algorithm \ref{GMDA}.
\\\\
\begin{algorithm}[H]
\label{GMDA}
\caption{Gaussian Mixture Discriminant Analysis}
\SetAlgoLined
	\KwIn {\textit{T}: Training set\; \textit{V}: Validation set\; $n_1$: The number of clusters for normal data\; $n_2$: The number of clusters for anomaly data\; $D$: The list of possible choices of $d$\; $L$: An empty list to restore \textit{F-score}s.}
	\KwOut {$f_1(X)$: Density of Normal Data\; $f_2(X)$: Density of Anomaly Data\; $d$: Threshold.}
		Apply EM algorithm to normal data in \textit{T} to obtain a density function $f_1(X)$ with $n_1$ clusters\;
		Apply EM algorithm to anomaly data in \textit{T} to obtain a density function $f_2(X)$ with $n_2$ clusters\;
		\For {$d$ in $D$}{
			Classify data in \textit{V}. If $\log f_2(X) - \log f_1(X) > d$, classify $X$ as anomaly, else normal\;
			Compute \textit{F-score} and add it into \textit{L}\;
		}
		$\rho \gets \arg\max L$\; 
		$d\gets D[\rho]$\;
		\Return $f_1(X), f_2(X), d$.
\end{algorithm}
 
\subsection{Relation and Extension with Other Methods}

\subsubsection{Relation with Naive Bayes}
\par

Naive Bayes models data using Gaussian distribution, which is similar to GMDA. However, there is much difference between the two methods.
\begin{enumerate}
\item For convenience of optimization, Naive Bayes use one Gaussian distribution, while we use Gaussian Mixture Model.
\item In Naive Bayes, a crucial assumption is independence between variables. However, we do not impose this assumption. In fact, the goal of using Gaussian Mixture Model is to capture the correlation between variables.
\end{enumerate}

\subsubsection{Relation with LDA and QDA}
\par

Both LDA and QDA (see Section \ref{DA}) model data from different classes seperately. Also, they embed prior probabilities from Bayesian view. However, there is difference between LDA/QDA and GMDA.
\begin{enumerate}
\item In LDA/QDA, we model the data using one Gaussian distribution, while in GMDA we model with Gaussian Mixture.
\item LDA/QDA use distribution of different classes to estimate prior probabilities. While in GMDA, we also embed loss of misclassification and use validation sets to tune the threshold. 
\end{enumerate}

\subsubsection{Relation with Robust Covariance Estimator(RCE)}
\par

A big difference between RCE and GMDA is that RCE only models one class of data, i.e., the normal data. Enlightened by RCE, we can combine the two algorithms with each other. We can use normal data in the training set to find a Gaussian Mixture model, and then directly tuning the threshold on validation set without further modelling anomaly data. This is GMDA with normal data only, ``GMDA-n".

\subsubsection{Relation with Decision Tree}
\par

If we look at Algorithm \ref{GMDA} again, we could get further insight. First, $\log f_2(X) - \log f_1(X)$ can be thought as a projection from $\mathbb{R}^n$ to $\mathbb{R}$. Second, the validation process to choose optimal $d$ can be thought as finding a tree with one layer that maximize the \textit{F-score}. This is equivalent to maximize 
\begin{eqt}
2\bigg/\left(\frac{\text{TP}+\text{FP}}{\text{TP}}+\frac{\text{TP}+\text{FN}}{\text{TP}}\right) = \frac{2\text{TP}}{2\text{TP}+\text{FP}+\text{FN}},
\end{eqt}
or equivalently, minimize
\begin{eqt}
\frac{\text{FP} + \text{FN}}{\text{TP}}.
\end{eqt}
\par
In Decision Tree setting with two classes, this is, to some extent, similar to minimizing the misclassification rate. This enlighten us to combine Algorithm \ref{GMDA} with Decision Tree Classifier.

\begin{algorithm}[H]
\label{GMDA-DTC}
\caption{GMDA with Decision Tree Classifier}
\SetAlgoLined
	\KwIn {\textit{T}: Training set\; \textit{V}: Validation set\; $n_1$: The number of clusters for normal data\; $n_2$: The number of clusters for anomaly data\;}
	\KwOut {$f_1(X)$: Density of Normal Data\; $f_2(X)$: Density of Anomaly Data\; \textit{DTC}: A Decision Tree Classifier.}
		Apply EM algorithm to normal data in \textit{T} to obtain a density function $f_1(X) = \sum_{i=1}^{n_1}\pi_if_{1, i}(X)$\;
		Apply EM algorithm to anomaly data in \textit{T} to obtain a density function $f_2(X) = \sum_{j=1}^{n_2}\pi_if_{2, j}(X)$\;
		Construct new features based on $f_1(X)$, $f_2(X)$, $f_{1, i}(X)$, $f_{2, j}(X)$$(1\leqslant i\leqslant n_1, 1\leqslant j\leqslant n_2)$\;
		Use the new features above to build a Decision Tree Classifier\;
		Tune the parameters using \textit{V}\;
		\Return $f_1(X), f_2(X)$, \textit{DTC}.
\end{algorithm}

\subsubsection{Relation with Ensemble Methods}
\par Once we could build a Decision Tree Classifier based on new features, we could embed it into ensemble algorithms. Bagging, voting, and XGBoosting are all excellent choices for enhancing performance, though parameters remain to be tuned to achieve satisfying results.

\section{Case Study: Credit Card Fraud Detection}

\subsection{Data Description}
\par We use an open dataset on Kaggle. The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions, which indicates that the number of frauds is small. The positive class (frauds) account for 0.172\% of all transactions, which means that the dataset is highly unbalanced.

The dataset contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, the original features and more background information about the data are not provided. 28 of 30 features are the principal components obtained with PCA, and the only features which have not been transformed with PCA are ``Time" and ``Amount". Feature ``Time" contains the seconds elapsed between each transaction and the first transaction in the dataset. Feature ``Amount" is the transaction amount. Feature ``Class" is the response variable and it takes value 1 if the transanction is fraud and 0 otherwise.

\subsection{Data Preprocessing}
\par We do not make further process on the 28 transformed features. As to Feature ``Time", we replace the time elasped between each transaction and the first transaction with the difference between each transaction and its former one. In practice, we do find that such operation generally enhances the performance of various algorithms.

\par We draw the histogram of each feature corresponding to the response variable. From the charts, we have some confidence that the assumption of our algorithm holds. The distribution can be well approximated by Gaussian Mixture. Of course, rigorous demonstration is left to numerical experiments in the following subsections.

\par As for data splitting, we set the proportion among training, validation, and test set as 6:2:2. If the training set is small, say, less than half of the whole dataset, then training or modelling is difficult, especially for anomaly data. If the validation set or test set is too small, then our evaluation metric will be vulnerable to a tiny change in anomaly prediction. Our choice is a tradeoff. It's worth noting that we keep the ratio between normal data and anomalies the same in all the 3 sets. This is essential for us to obtain a fair result.

\subsection{Implementation Details}
\par In this section, we will discuss the implementation of previously described algorithms, and make comparison among them from experiments. These numerical experiments are carried out on a computer with 2 Intel Core i7-6500U CPUs and 16GiB RAM.

\subsubsection{Supervised Learning Methods}
\par For supervised learning methods, we have implemented several methods. The setting of these methods are described below.
\begin{itemize}
	\item\textbf{Decision Tree(DTC): } We choose the criterion for splitting nodes and pruning the tree to be Gini index, and the maximum depth of the tree to be 6. Furthermore, we have done experiments with weighted tree, whose purpose is to balance the number of instances in each class. We have chosen two type of weights: $1:5$ and $1:10$.
	\item\textbf{LDA with bagging(LDA-Bag): }We choose the number of bagging LDA estimators to be 5.
	\item\textbf{QDA with bagging(QDA-Bag): }We choose the number of bagging QDA estimators to be 11.
	\item\textbf{Random Forest(RF): } We choose the criterion for splitting nodes and pruning the tree to be Gini index, and the number of estimators to be .
	\item\textbf{XGBoost(XGB): }We choose the maximum depth of each tree to be 4, regularization term $\lambda$ to be 0.5 and $\gamma$ to be 0.
	\item\textbf{Neural Network(NN): }We choose the structure of the neural network to be a 4-layer fully connected neural network, with 5, 4, 3, 3 neurons in each layers respectively.
\end{itemize}

\subsubsection{Outlier Detection Methods}
\par For outlier detection methods, we have implemented three algorithms mentioned above. The parameters are as follows.
\begin{itemize}
\item \textbf{Robust Covariance Estimation(RCE): } We directly run the fitting process of the algorithm on the training set, which contains both normal and anomaly data. We then tune the threshold for $d$ (see \ref{mahadist}) using validation set. The optimal threshold we select is $1.01 \times 10^6$. 
\item \textbf{Local Outlier Factor(LOF): } We set the number of neighbors $k$ to be 50 and the proportion of outliers as 0.0008. During the training process, we only use the normal data in the training set because we observe that directly running the fitting process on the whole training set will worsen the performance.
\item \textbf{Isolation Forest(ILF): } We set the number of trees to be 250 and the proportion of outliers as 0.005. We only use the normal data in the training set, as what we have done in Local Outlier Factor. 
\end{itemize}
\subsubsection{GMDA and Related Methods}
\par We have also implemented several algorithms related to GMDA. We now explain them as follows.
\begin{itemize}
\item \textbf{Gaussian Mixture Discriminant Analysis(GMDA): } We model normal data with $n_1=3$ clusters and anomaly data with $n_2=3$ clusters in the training set. Note that all of the following methods are based on the two fitted gaussian mixture (normal and anomaly). 
\item \textbf{Gaussian Mixture Discriminant Analysis with normal data only(GMDA-n): } We use the fitted distribution of normal data obtained by GMDA and then tune a threshold using the validation set.
\item \textbf{Gaussian Mixture with Decision Tree Classifier(GM-dtc): } The algorithm is consistent with \ref{GMDA-DTC}. The features we select are
\begin{eqt}
\label{feature}
f_1(X) - f_2(X), \ f_{1, i}(X), \ f_{2, j}(X)\ (1\leqslant i\leqslant n_1, 1\leqslant j\leqslant n_2).
\end{eqt}
We choose Entropy Loss as the criterion. To prevent overfitting, we restrict the maximal depth of the tree to 5. 
\item \textbf{Gaussian Mixture with Bagging(GM-bag): } This algorithm is based on GM-DTC. We choose 11 tree estimators with the same features in (\ref{feature}). 
\item \textbf{Gaussian Mixture with Voting(GM-vote): } This algorithm is based on GM-DTC. To increase diversity, we choose 9 different tree estimators with the same features in (\ref{feature}), but with different weights(see Section \ref{weightdtc}), which are
\begin{eqt}
100:1,\ 50:1,\ 20:1,\ 10:1,\ 1:1,\ 1:10,\ 1:20,\ 1:50,\ 1:100.
\end{eqt}
\item \textbf{Gaussian Mixture with XGBoost(GM-xgb): } This algorithm is based on GM-DTC. We choose the maximal depth of each tree to be 5, regularization term $\lambda$ to be 0.5, and $\gamma$ to be 0.

\end{itemize}

\subsection{Comparison between Different Methods}
\par We have implemented several previous described algorithms together with algorithms proposed by ourselves, and the results can be viewed in Table \ref{comparison}.
\par From this table, we observe that tree-based algorithms works better than other supervised learning methods. However, some tree based methods also suffer from relatively low recall. This is due to the fact that anomaly data is very rare, and thus tree-based classifiers tend to classify the data into the normal class. Apart from tree-based methods, from the numerical experiments, other supervised learning methods seldom rival the tree-based methods, and lots of these methods suffer from the problem we discussed before.
\par When talking about the weighted tree, the effect of the previously describe problem is reduced to some extent. Usually the value of recall increases a little bit, while the value of precision reduces some. The F-score, which is a balance between recall and precision, almost remains the same. Hence we can adjust the weight used in the weighted tree based on our need for recall and precision.
\par As for ensemble learning technique, in general, these methods can improve the performance of basic methods. Such as the random forest methods and the XGBoost methods perform better than the decision tree algorithms. However, one drawback of these ensemble methods is that they take much longer time than basic algorithms. The time cost of XGBoost is nearly nine times bigger than the simple decision tree. And as for the bagging methods, the time cost of them nearly equals to the time cost of basic methods times the number of estimators. Hence even though these methods can usually reduce the variance of training and sometimes lead to robust models, their high time cost certainly brings disappointed effect on them.
\par Some other supervised learning methods, such as k-nearest neighbor methods and support vector classifier methods, are not included in our numerical experiments due to the extremely high time cost in running them. (These methods can only be used in datasets with small number of instances.)
\par In contrast, the Gaussian mixture model methods seems to be affected by this reason less likely. These Gaussian models extract the structure of normal and anomaly data correctly. And using these Gaussian models instead of raw data to make prediction can seriously reduce the effect of data imbalance. And from the table, we can notice that all Gaussian mixture models works pretty well.
\par Furthermore, when it comes to outlier detection methods, their results are not good enough. The main reason for this is the wrong assumptions we have made. Outlier detection methods usually rely on the assumption that outliers are instances outside the normal data distribution, and there is no explicit distribution for them. But in this dataset, the distribution of outliers seems to have certain structure, which contradicts to the underlying assumption of outlier detection methods.

\begin{table}[htbp]
	\centering
	\setlength{\belowcaptionskip}{10pt}
	\begin{tabu}{|c|c|c|c|c|c|c|c|c|c|c|c|}
	\tabucline[1.5pt]{-}
	\multirow{2}{*}{Methods} & \multicolumn{3}{|c|}{Train} & \multicolumn{3}{|c|}{Valid} & \multicolumn{3}{|c|}{Test} & \multicolumn{2}{|c|}{Time}\\
	\cline{2-12}
	& R & P & F & R & P & F & R & P & F & T & P\\
	\tabucline[1.5pt]{-}
Logistic & 58.6 & 85.2 & 64.9 & 61.2 & 88.2 & 67.6 & 59.6 & 86.8 & 66.0 & 8.88 & 0.01\\
\hline
NB & 82.4 & 5.9 & 16.5 & 84.7 & 6.1 & 17.1 & 82.8 & 5.8 & 16.2 & 0.09 & 0.02\\
\hline
DTC & 83.4 & 98.4 & 87.5 & 81.6 & 90.9 & 84.3 & 79.8 & 91.9 & 83.2 & 4.02 & 0.01\\
\hline
DTC 1:5 & 84.6 & 97.8 & 88.3 & 80.4 & 91.3 & 83.5 & 78.2 & 92.2 & 82.0 & 4.19 & 0.01\\
\hline
DTC 1:10 & 85.8 & 94.1 & 88.2 & 83.4 & 86.0 & 84.2 & 79.8 & 86.2 & 81.7 & 4.16 & 0.01\\
\hline
LDA & 74.2 & 85.5 & 77.4 & 82.7 & 88.0 & 84.2 & 77.8 & 87.5 & 80.5 & 0.70 & 0.01\\
\hline
LDA-Bag & 73.8 & 85.4 & 77.1 & 81.8 & 87.9 & 83.6 & 77.1 & 87.3 & 80.0 & 4.20 & 0.02\\
\hline
QDA & 87.5 & 5.4 & 15.4 & \textbf{90.8} & 5.6 & 16.1 & 84.8 & 5.3 & 15.1 & 0.34 & 0.03\\
\hline
QDA-Bag & 87.4 & 6.0 & 16.9 & 90.7 & 6.3 & 17.6 & \textbf{84.9} & 5.9 & 16.6 & 5.30 & 0.27\\
\hline
RF & \textbf{94.4} & \textbf{99.6} & \textbf{95.9} & 80.0 & 92.8 & 83.5 & 77.1 & \textbf{94.1} & 81.6 & 11.78 & 0.06\\
\hline
NN & 78.0 & 86.5 & 80.4 & 82.7 & 88.0 & 84.2 & 80.8 & 85.1 & 82.1 & 7.54 & 0.01\\
\hline
XGBoost & 87.5 & 99.2 & 90.8 & 82.7 & \textbf{94.2} & 85.9 & 80.8 & \textbf{94.1} & \textbf{84.5} & 37.29 & 0.13\\
\tabucline[1.5pt]{-}
RCE & 74.9 & 3.5 & 10.4 & 79.6 & 3.9 & 11.5 & 80.8 & 3.8 & 11.3 & 20.19 & 0.05 \\
\hline
LOF & 50.8 & 54.5 & 51.9 & 55.1 & 53.5 & 54.6 & 50.5 & 51.5 & 50.8 & 368.53 & 122.77 \\
\hline
ILF & 47.5 & 14.1 & 27.5 & 46.9 & 13.6 & 26.8 & 47.5 & 14.3 & 27.7 & 368.53 & 3.96 \\
\tabucline[1.5pt]{-}
GMDA & 79.0 & 80.3 & 79.4 & 85.7 & 81.6 & 84.4 & 80.8 & 82.5 & 81.3 & 55.55 & 0.11 \\
\hline
GMDA-n & 77.6 & 82.1 & 78.9 & 84.7 & 83.0 & 84.2 & 79.8 & 84.9 & 81.3 & 6.69 & 0.06 \\
\hline
GM-dtc & 82.7 & 92.4 & 85.5 & 83.7 & 85.4 & 84.2 & 78.8 & 89.7 & 81.8 & 1.99 & 0.20 \\
\hline
GM-bag & 83.4 & 94.6 & 86.5 & 84.7 & 90.2 & 86.3 & 79.8 & 92.9 & 83.4 & 8.23 & 0.25 \\
\hline
GM-vote & 82.0 & 95.3 & 85.7 & 84.7 & 88.3 & 85.8 & 78.8 & 94.0 & 82.9 & 7.95 & 0.25 \\
\hline
GM-xgb & 86.8 & 98.8 & 90.2 & 84.7 & 91.2 & \textbf{86.6} & 80.8 & \textbf{94.1} & \textbf{84.5} & 13.88 & 0.33 \\
\tabucline[1.5pt]{-}
	\end{tabu}
	\caption{Comparison between different methods. Each row represent one kind of algorithm. The first eleven rows are about supervised learning method, and the next six rows are about Gaussian mixture model methods, and the last three rows are about outlier detection methods. We have presented the recall (R), precision (P) and F-score (F) on the training, validation and test sets above, where the definition of recall, precision and F-score are given in the first section. The last two columns are about time costs on training process and prediction process. (on test set)}
	\label{comparison}
\end{table}

\section{Conclusion}
\par Our work for Outlier Classification ends up here, but new ideas for improvement and continuous passion for research will never fade. We now give a brief summary for what we have done and give some outlooks. 

\par The Outlier Classification mainly consists of two challenges. The first one is that highly unbalanced dataset may lead to deterioration of standard supervised learning technique. The second one is that the small number of outliers(anomalies) may impose annoying influence on our evaluation performance. We propose several techniques to deal with this problem. On the side of evaluation, we restrict ourselves on \textit{recall}, \textit{precision} and a weighted \textit{F-score} as our evaluation metric. To deal with the potential unstable results, we propose to run several times of our algorithms and obtain the average.

\par On the side of algorithms, based on Bayesian decision theory, we propose GMDA to utilize the advantage of clustering methods and to capture the different distribution between normal and anomaly data. From the view of Naive Bayes/LDA/QDA, we extend the modelling of distribution to Gaussian Mixture, and regard some parameters to be tuned automatically as thresholds by the algorithm. From the view of Decision Tree Classifier and other ensemble methods, the algorithm deal with raw data to obtain highly nonlinear features to enhance performance. The algorithm can also be combined with several other supervised learning techniques.

\par As to numerical experiments, we conduct a case study to compare our methods with standard ones. Surprisingly, tree-based methods perform relatively well on this setting, which indicates that the unbalanced dataset does not affect peformance too much when we use trees. Our algorithms perform quite well on this dataset. Of course, the success of our algorithms may, to some extent, depend on the organized shape of the dataset. It's worth noting that traditional outlier detection methods perform badly in our setting. This may be caused by lack of information of data labels.

\par Now we provide some outlooks. For supervised learning methods, we want may further investigate some combined methods such as kernel smoothing. Due to the time limit, we do not implement them. Furthermore, deep-learning based methods are prospective. After all, from our experiments, a simple neural network performs comparable to the best performed algorithms. For our proposed algorithms, apart from directly geenrating features and embed them into trees, we could also split trees first and then apply Gaussian Mixture, though this is left to future work. We also want to mention that for outlier detection methods, we have potential for trying with other algorithms in the literature, since the three algorithms we implemented are the most standard and the simplest.

\par As we have mentioned in Introduction, Outlier Classification is very common in many business setting. However, there is still room for improvement even with numerous proposed methods. Of course, we hope that we can find more efficient methods, both in time and space. This will be left for future research.

\bibliography{reference}
\bibliographystyle{plain}

\begin{appendix}
\section{Proof of Theorem \ref{optbayes}}
\par

We have
\begin{eqt}
g(D^*) & = \sum_{i=1}^kp_i\sum_{j=1}^kP(j|i;D^*)L(j|i) \\
& = \sum_{i=1}^kp_i\sum_{j=1}^kL(j|i)\int_{D_j^*}f_i(X)dX \\
& = \sum_{j=1}^k\int_{D_j^*}\sum_{i=1}^kp_iL(j|i)f_i(X)dX \\
& = \sum_{j=1}^k\int_{D_j^*}h_j(X)dX.
\end{eqt}
For any partition of $\mathbb{R}^n$, $D = \{D_1, ..., D_k\}$, we also have
\begin{eqt}
g(D) = \sum_{j=1}^k\int_{D_j}h_j(X)dX.
\end{eqt}

Therefore, 
\begin{eqt}
g(D^*) - g(D) & = \sum_{j=1}^k\int_{D_j^*}h_j(X)dX - \sum_{t=1}^k\int_{D_t}h_t(X)dX \\
& = \sum_{j=1}^k\sum_{t=1}^k\int_{D_j^*\cap D_t}\left(h_j(X)-h_t(X)\right)dX \\
& \leqslant 0.
\end{eqt}
In fact, $\forall j, t\in[k]$, we have $h_j(X)\leqslant h_t(X)$ on $D_j^*\cap D_t$ from the definition of $D^*$. This completes the proof.
\end{appendix}

\end{document}
