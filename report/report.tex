\documentclass[english]{article}
\RequirePackage{fancyhdr}
\usepackage{times}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{lastpage}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{indentfirst}
\newenvironment{eqt}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newtheorem{defn}{Definition}[section]
\newtheorem{thm}{Theorem}[section]


\fancypagestyle{plain}{
	\fancyhf{}\renewcommand*{\headrulewidth}{0.75bp}
	\fancyhead[R]{\normalfont{Page \thepage{} of \pageref*{LastPage}}}
	\fancyhead[L]{\normalfont{\cuniversity}}
	\fancyhead[C]{\normalfont{\titlemark}}
}
\pagestyle{plain}

\geometry{
	a4paper, includefoot, hmargin = 2.6cm, top = 2.7cm, bottom = 2cm,
	headheight = 1.5cm, headsep = 0.5cm, footskip = 0.75cm
}

\DeclareRobustCommand{\authorthing}{
\begin{tabular}{cc}
\large{Zeyu Jia}\thanks{The authors are arranged lexicographically.} & \large{Feng Zhu}\footnotemark[1]\\
1600010603 & 1600010643
\end{tabular}\\
\\
School of Mathematical Science, Peking University
}

\newcommand{\cuniversity}{Peking University}
\newcommand{\titlemark}{A Brief Survey on Outlier Classification}
\newcommand{\Input}{\textbf{Input}}
\newcommand{\Output}{\textbf{Output}}
\author{\authorthing}
\title{\textbf{\titlemark}}

\begin{document}
\maketitle

\abstract{12}

\section{Introduction}
\par Anomaly detection (or outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, anomalous data can be connected to some kind of problem or rare event such as, e.g., bank fraud, medical problems, structural defects, etc. This connection makes it very interesting to be able to pick out which data point can be considered anomalies, as identifying these events are typically essential from a business perspective. There have been plentiful literatures on machine learning methods for outlier detection, and they generally focus on unsupervised or semi-supervised ones. For a review of them, readers could refer to \cite{review}.
\par However, unlike labelling of pictures or videos, it is not that difficult to label data points in anomaly detection setting. For example, in financial fraud detection, it's easy to obtain whether a deal is fraud or not after it is done. We observe that labeled data for outlier detection is always available for training and testing numerous algorithms. Why do we have to apply unsupervised or semi-supervised learning methods given that the labels are complete? Thus, in this paper, we will mainly focus on ``Outlier Classification", where labels are all presented.
\indent Outlier Classification gives rise to other challenging problems. 
\begin{enumerate}
\item The dataset is highly imbalanced. The ratio between normal data and anomaly ones may be very large, sometimes exceeding 500:1. This may make traditional classification techniques ineffective.
\item The number of anomaly data is small, even if the size of the whole data set is large. The problem become especially serious when we split the dataset into several parts, because it imposes high volatility on evaluation with regard to anomaly data.
\end{enumerate} 
\par To deal with the problems above, we need to clarify our evaluation metric before going further. We will use \textit{recall}(R) and \textit{precision}(P), which is defined as
\begin{eqt}
\textit{recall}\text{(R)} & = \frac{\text{TP}}{\text{TP}+\text{FN}},\\ 
\textit{precision}\text{(P)} & = \frac{\text{TP}}{\text{TP}+\text{FP}}.
\end{eqt}
To integrate the two metric, we also define our \textit{F-score}. In practice, it is always more important to detect all true anomalies as much as possible, which means that we should put more weight on \textit{recall} than \textit{precision}. Our \textit{F-score}(F) is defined as
\begin{eqt}
\textit{F-score}\text{(F)} = \frac{2.5\times \text R\times \text P}{1.5\times \text P + \text R}.
\end{eqt}
\par Our paper is organized as follows. Section 2 will briefly introduce several supervised learning techniques. Among them, we pay special attention to (weighted) Decision Tree Classifier, XGBoost, and Ensemble methods. Section 3 will briefly introduce three commonly used unsupervised learning techniques for outlier detection. In Section 4, we will introduce our algorithm, ``GMDA", which is originally based on Bayesian decision theory, to combine both supervised and unsupervised learning methods. We also explain its relation and extension with other methods. We conduct a case study of financial fraud detection in Section 5 to produce plentiful results and compare different algorithms. In Section 6, we conclude. The division of our work is in Acknowledgement.

\section{Supervised Learning Models}
\par In this section, we will discuss several commonly used supervised learning methods, and their application on outlier classification tasks. These supervised learning methods includes
\begin{itemize}
	\item Logistic regression classifier,
	\item Gaussian Naive bayes classifier,
	\item Decision tree classifier,
	\item Support vector classifier,
	\item Discriminant analysis classifier,
	\item K-nearest neighbor classifier.
\end{itemize}
\par Furthermore, we will also introduce two useful techniques used in classification problem: boosting methods, and random forest/bagging/voting methods. These methods and techniques refer to \cite{scikit-learn}\cite{friedman2001elements}, and will be discussed in the following subsections respectively.

\subsection{Logistic Regression Classifier}
\par The logistic regression classifier is one of the most simple methods used in 2-class classification problem. Suppose two class are denoted by $0$ and $1$. Then we will find an proper $W$ and $b$ such that
\begin{equation}\label{logistic}
	\begin{aligned}
		&P(y=0|X=x, \beta=(W, b)) = \frac{1}{1 + \exp(Wx+b)},\\
		&P(y=1|X=x, \beta=(W, b)) = \frac{\exp(Wx+b)}{1 + \exp(Wx+b)},\\
	\end{aligned}
\end{equation}
and we will maximize the log-likelihood:
\begin{equation}
	l(\beta) = \sum_{i=1}^{N}y_{i}\log P(y=y_{i}|X=x_{i}, \beta) + (1-y_{i})\log P(y\neq y_{i}|X=x_{i}, \beta).
\end{equation}
By finding the optimal parameter $\beta$, then we can use \eqref{logistic} to make prediction.

\subsection{Gaussian Naive Bayes Classifier}
\par Gaussian naive Bayes classifier is one of Bayesian classifier using Gaussian distribution as prior. We make the assumption that
\begin{equation}
	P(X|y) = \prod_{i=1}^{m}P(x_{i}|y),
\end{equation}
where $X = (x_{1}, \cdots, x_{m})$ is the feature vector. This assumption talks about the independence of each element of feature vector. And then we can use MAP to make classification:
\begin{equation}\label{GNB}
	\hat{y} = \arg\max_{y}P(y|X) = \arg\max_{y}\frac{P(y)\prod_{i=1}^{m}P(x_{i}|y)}{P(X)} = \arg\max_{y}P(y)\prod_{i=1}^{m}P(x_{i}|y).
\end{equation}
And here we assume the likelihood given each $y$ to be Gaussian:
\begin{equation}
	P(x_{i}|y)\sim\mathcal{N}(\mu_{y}, \sigma_{y}),
\end{equation}
where parameters $\mu_{y}$ and $\sigma_{y}$ are determined by maximum likelihood estimation. After finishing this estimation, the classification process can be done according to \eqref{GNB}.
\par The underlying assumption of this method is that data in each class satisfies Gaussian distribution, and each features are independent given its label.

\subsection{Decision Tree Classifier}
\par The decision tree classifier is a little bit different to other methods. The other methods all aim to find a proper linear (or nonlinear) combination of features. The decision tree, however, will execute on features sequentially. In other words, in each step of decision tree's iteration, only one feature will be taken into consideration.
\par The training of a decision tree is often divided into two parts: splitting and pruning. In the splitting part, the decision tree will expand their leaves. That is, finding a new features (often the one with the most reduction of the loss) and then split one of the tree leaf into two new leaves. In the pruning process, some of leaves will merged into one, if merging them can decrease the value of loss plus regularization.

\subsubsection{Weighted Decision Tree}
\label{weightdtc}
\par Weighted decision tree is a useful model in tackling cases where number of instances in each class varies a lot.
\par To balance the number of instances in each class, and to let the classifier be not prone to classify new instance into the major class, we multiply instances in each class by a weight. After doing that, we aim to let instance weight in each class multiplied by the number of instances in that class not vary a lot. (This has the same effect as duplicating instances in minor classes several times.) Then we will train a decision tree with these instances and weights.
\par Adopting this technique, the inclination to classify a new instance to major classes reduces a lot.

\subsection{Support Vector Classifier}
\par The support vector classifier is also termed with support vector machine for classification. 
\par Estimating parameters of common SVM (linear SVM) can be done by solving the following optimization problem:
\begin{equation}
	\begin{aligned}
		& \min_{\beta, \beta_{0}}\frac{1}{2}\|\beta\|^{2} + C\sum_{i=1}^{n}\xi_{i}\\
		& \text{subject to }\xi_{i}\ge 0, y_{i}(x_{i}^{T}\beta + \beta_{0})\ge 1 - \xi_{i}, \quad \forall i
	\end{aligned}
\end{equation}
\par We can solve it by writing its dual problem:
\begin{equation}
	\begin{aligned}
		& \max_{\alpha_{i}}\sum_{i=1}^{n}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j},\\
		& \text{subject to } 0\le \alpha_{i}\le C, \quad\sum_{i=1}^{n}\alpha_{i}y_{i} = 0.
	\end{aligned}
\end{equation}
\par In order to handle data that cannot be linear separated, we introduce the kernel function, which elevate the feature $x_{i}$ into $h(x)$. If $\langle h(x), h(y)\rangle = K(x, y)$, where this $K(x, y)$ is called the kernel, then the dual problem becomes
\begin{equation}
	\begin{aligned}
		& \max_{\alpha_{i}}\sum_{i=1}^{n}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i}, x_{j}),\\
		& \text{subject to } 0\le \alpha_{i}\le C, \quad\sum_{i=1}^{n}\alpha_{i}y_{i} = 0,
	\end{aligned}
\end{equation}
and the prediction step can be written as
\begin{equation}
	f(x) = h(x)^{T}\beta + \beta_{0} = \sum_{i=1}^{n}\alpha_{i}y_{i}K(x, x_{i}) + \beta_{0}.
\end{equation}
We can classify the instance $x$ according to the sign of $f(x)$.
\par In our implementation, we use the RBF kernel 
\begin{equation}
	K(x, y) = \exp\left(-\frac{\|x-y\|^{2}}{2\sigma^{2}}\right)
\end{equation}
to make classification.

\subsection{Discriminant Analysis Classifier}
\label{DA}
\par The discriminant analysis classifiers we implemented include the linear discriminant analysis classifier (LDA) and quadratic discriminant classifier (QDA).
\subsubsection{Linear Discriminant Analysis Classifier}
\par In LDA setting, the underlying assumption is that data in each class satisfies Gaussian distribution. Furthermore, we also assume the covariance matrix in each class shares the same one. 
\par To use this model to classify a new instance, firstly, we need to estimate the probability of each class $\hat{\pi}_{k}$, the mean of the data distribution of each class $\hat{\mu}_{k}$, and the covariance matrix $\Sigma$:
\begin{itemize}
	\item $\hat{\pi}_{k} = N_{k} / N$, where $N_{k}$ is the number of instances in class $k$ and $N$ is the total number of instances. ($k = 1, 2$),
	\item $\hat{\mu}_{k} = \sum_{y_{i} = k}x_{i} / N_{k}$,
	\item $\hat{\Sigma} = \sum_{k=1}^{2}\sum_{y_{i} = k}(x - \hat{\mu}_{k})(x - \hat{\mu}_{k})^{T} / (N - 2)$.
\end{itemize}
Then we can use the following quantity
\begin{equation}
	\delta_{k}(x) = x^{T}\Sigma^{-1}\mu_{k} - \frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k} + \log\pi_{k}
\end{equation}
to determine the estimated class of instance $x$, according to
\begin{equation}
	\hat{y} = \arg\max_{k\in\{1, 2\}}\delta_{k}(x).
\end{equation}

\subsubsection{Quadratic Discriminant Analysis Classifier}
\par In LDA's setting, we assume the covariance matrix in each class to be the same one, but sometimes this assumption cannot hold, and we need to estimate the covariance matrix for each class. This estimation is called quadratic discriminant analysis.
\par The estimating method for the covariance in class $k$ is
\begin{equation}
	\hat{\Sigma}_{k} = \frac{1}{N_{k} - 1}\sum_{y_{i} = k}(x_{i} - \hat{\mu}_{k})(x_{i} - \hat{\mu}_{k})^{T},
\end{equation}
where $N_{k}$ is the total number of training data classified into class $k$, and $\mu_{k}$ is the estimated mean according to the formula in LDA.
\par In QDA setting, we can use the following quantity
\begin{equation}
	\delta_{k}(x) = -\frac{1}{2}\log|\hat{\Sigma}_{k}| - \frac{1}{2}(x - \mu_{k})^{T}\Sigma_{k}^{-1}(x - \mu_{k}) + \log\pi_{k}
\end{equation}
to determine the estimated class of instance $x$, according to
\begin{equation}
	\hat{y} = \arg\max_{k\in\{1, 2\}}\delta_{k}(x).
\end{equation}
\par Note that the QDA classifier is similar to the naive Bayes classifier, only lack of the assumption that each feature is independent given the class label. Hence if we assume the covariance matrices $\hat{\Sigma}_{k}$ to be diagonal, then we will obtain the same results to naive Bayes classifier.

\subsection{K-Nearest Neighbor Classifier}
\par The K-nearest-neighbor classifier (KNN classifier) is the simplest kernel smoothing classifier. Its idea is to label a new instance according to the majority among its nearest $k$ instances label.
\par Specifically, suppose the training set is denoted by $\{(x_{i}, y_{i})\}_{1\le i\le n}$ ($y_{i}\in\{0, 1\}$), and we have a new instance $x$. If the $k$ nearest neighbors to $x$ among $x_{i}$ are $x_{1}\cdots, x_{k}$, then we will label it according to
\begin{equation}
	\hat{y} = \begin{cases}
		0 & \quad \text{if }\sum_{i=1}^{k}y_{i}\le k / 2,\\
		1 & \quad \text{if }\sum_{i=1}^{k}y_{i}> k / 2.\\
	\end{cases}
\end{equation}
\par The training process of the KNN method takes very short time. However, in the predicting process, for every new instance, we need to search over the training set to find $k$ nearest neighbor to the new instance, which will be quite time costly if the training set is extremely large.

\subsection{Boosting Methods}
\par We will discuss two boosting methods we have implemented in this subsection:
\begin{itemize}
	\item Adaboost
	\item XGboost
\end{itemize}

\subsubsection{Adaboost}
\par Adaboost method is one of the most commonly used ensemble method aiming to improve the accuracy on the training set.
\par The adaboost method can be viewed as a method which improves the performance on the training set by training a model with larger weight on data misclassified and smaller weight on data classified correctly.
\par Specifically, suppose we have $m-1$ classifier $G_{1}, \cdots, G_{m-1}$ with weight $\beta_{1}, \cdots, \beta_{m-1}$. And we have function $f_{i}$ with $f_{i}(x) = f_{i-1}(x) + \beta_{i}G_{m}(x)$. Then we will train the $m$-th classifier $G_{m}$ and its weight $\beta_{m}$ to minimize
\begin{equation}
	\beta_{m}, G_{m} = \arg\min_{\beta, G}\sum_{i=1}^{n}\exp(-y_{i}(f(x_{i}) + \beta G(x_{i}))),
\end{equation}
and then we let
\begin{equation}
	f_{m}(x) = f_{m-1}(x) + \beta_{m}G_{m}(x).
\end{equation}

\subsubsection{XGboost}
\par This part refers to the paper \cite{chen1603xgboost}. XGboost is the abbreviation of ``extreme gradient boosting'' method. This is also a method using the boosting tree technique. But instead of using gradient boosting, the main idea of XGboost is to use both the first and the second order derivative to approximate the loss function.
\par Specifically speaking, we first define the loss function of the model:
\begin{equation}
	\mathcal{L} = \sum_{i=1}^{n}l(y_{i}, \hat{y}_{i}) + \sum_{k=1}^{K}\Omega(f_{k}),
\end{equation}
where $K$ is the number of trees in this model, $\Omega$ is the regularization function, and $\hat{y}_{i}$ is defined as
\begin{equation}
	\hat{y}_{i} = \sum_{k=1}^{K}f_{k}(x_{i}).
\end{equation}
\par When we aim to train the $t$-th tree $f_{t}$, the loss function is 
\begin{equation}
	\mathcal{L}^{(t)} = \sum_{i=1}^{n}l(y_{i}, \hat{y}_{i}^{(t-1)} + f_{t}(x_{i})) + \Omega(f_{t}) + const,
\end{equation}
where $\hat{y}_{i}^{(t-1)}$ is the predicted value on $x_{i}$ according to the first $t-1$ trees. Next, in order to minimize this loss, we find the Taylor expansion:
\begin{equation}\label{xgboost}
	\mathcal{L}^{(t)}\approx\sum_{i=1}^{n}\left[l(y_{i}, \hat{y}_{i}^{(t-1)}) + g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})\right] + \Omega(f_{t}) + const,
\end{equation}
where
\begin{equation}
	g_{i} = \partial_{\hat{y}^{(t-1)}}l(y_{i}, \hat{y}^{(t-1)}), \quad h_{i} = \partial^{2}_{\hat{y}^{(t-1)}}l(y_{i}, \hat{y}^{(t-1)}).
\end{equation}
\par Normally, we choose the function $l$ as the squared loss function
\begin{equation}
	l(y_{i}, \hat{y}_{i}) = (y_{i} - \hat{y}_{i})^{2}.
\end{equation}
Then it is easy to calculate 
\begin{equation}
	g_{i} = 2(\hat{y}^{(t-1)} - y_{i}), \quad h_{i} = 2.
\end{equation}
\par Furthermore, the regularization term $\Omega(f)$ is often chosen to be
\begin{equation}
	\Omega(f) = \gamma T + \frac{\lambda}{2}\sum_{j=1}^{T}\omega_{j}^{2},
\end{equation}
where $T$ is the number of leaves in the tree $f$, and $\omega_{j}$ is the corresponding score with respect to node $j$.
\par In order to find the most proper tree model $f_{t}$, we only need to find the optimizer of \eqref{xgboost}.

\subsection{Random Forest / Bagging / Voting Methods}
\par Methods discussed in this subsection are similar in that they all adopt the idea of bagging or voting to make the classifier more powerful. We will discussed voting method, bagging method and the random forest method here in this subsection.

\subsubsection{Voting Method}
\par The voting method is a method balancing different classifiers in order to achieve a better classifier.
\par Suppose their are $n$ classifier $f_{1}, \cdots, f_{m}$. Then we assign weights $w_{1}, \cdots, w_{m}$ to each classifier. Then the idea of voting is to choose the label which is most likely been classified. That is, we choose 
\begin{equation}
	\hat{y} = \max_{y}\#\{f_{m}(x) = y, 1\le i\le m\}.
\end{equation}

\subsubsection{Bagging Method}
\par The bagging method adopts the idea of voting method, uses one model to train several classifiers by bootstrap, and then use these classifiers to vote to get the prediction. This type of methods can reduce the variance, and is easy for parallel computing.

\subsubsection{Random Forest Method}
\par The well-known random forest method is based on bagging method, but random forest goes one step further. In this method, the basic model is chosen to be decision tree, and in the bootstrap step, not only does the data applied bootstrap technique, but also the features are selected randomly.

\section{Outlier/Novelty Detection Methods}

\par
In this section, we will introduce some standard outlier detection methods. All these methods assume loss of labels and try to empirically figure out whether a collected data point is an ``outlier"(Outlier Detection) or an new data point is a ``novelty"(Novelty Detection). 

\subsection{Robust Covariance Estimator}
\par
The estimator is also called Minimum Covariance Determinant (MCD) estimator. It is, to some extent, similar to LDA. However, since it assumes lack of label, the algorithm only fits one Gaussian distribution to model the normal data. To fit the data robustly, it tries to find 
\begin{eqt}
\frac{n_{\textit{samples}}+n_{\textit{features}}+1}{2}
\end{eqt}
observations whose empirical covariance has the smallest determinant and yields a ``pure" subset of observations from which we can estimate so-called Mahalanobis distance for any data point $x$, i.e., 
\begin{eqt}
\label{mahadist}
d(x; \mu, \Sigma) = (x-\mu)'\Sigma^{-1}(x-\mu).
\end{eqt}
\par
If it's value is too large, then $x$ is classified as an anomaly, else a normal data point. For more detailed description, readers could refer to \cite{robustcov}.


\subsection{Isolation Forest}
\par
The algorithm borrows ideas form Random Forest. It first randomly select a feature, and randomly select a splitting value between the maximum and minimum values of the selected feature. Then it recursively conduct this process to form a tree. The number of splittings to isolate a sample is equivalent to the path length from the root node to the terminating node. Finally, the path length is averaged over a forest of such random trees, and embedded in an anomaly score, which is regarded as a measure of normality. The anomaly score in this algorithm is defined as 
\begin{eqt}
s(X, n) = 2^{-\frac{E[h(X)]}{c(n)}},
\end{eqt}
where $h(X)$ is the path length of observation $X$, $E[h(X)]$ is the average of $h(X)$ through the forest of trees, $c(n)$ is the average path length of unsuccessful search in a Binary Search Tree and $n$ is the number of external nodes. More on the anomaly score and its components can be found in \cite{isolationforest}.
\par
In fact, random partitioning produces shorter paths for anomalies because anomalies have high probability of being close to the extreme values of a feature. Hence, if for a particular sample, the forest of random trees collectively produce shorter path lengths, it is highly likely to be an anomaly.


\subsection{Local Outlier Factor}
\par
The algorithm computes a score (called local outlier factor) to reflect the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.
\par
Commonly, the local density is obtained from KNN. The LOF score of an observation is equal to the ratio of the average local density of his $k$-nearest neighbors and its own local density. A normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.
\par
Now let us describe the algorihm more rigirously. Denote $N_k(X)$ as the set of the first $k$ nearest neighbors of data point $X$ and $d(X, Y)$ as the distance between $X$ and $Y$. Define $d_k(X)$ as the distance between $X$ and its $k$th nearest neighbor. The reachability distance between $X$ and $Y$ is defined as
\begin{eqt}
\textit{rd}_k(X, Y) = \max\{d_k(Y), d(X, Y)\}.
\end{eqt}
The local reachability of $X$ is defined as 
\begin{eqt}
\textit{lrd}_k(X) = \frac{|N_k(X)|}{\sum_{Y\in N_k(X)}\textit{rd}_k(X, Y)}.
\end{eqt}
The local outlier factor of $X$ is defined as 
\begin{eqt}
\textit{lof}_k(X) = \frac{\sum_{Y\in N_k(X)}\textit{lrd}_k(Y)}{|N_k(X)|\textit{lrd}_k(X)}.
\end{eqt}

\paragraph{}
If $\textit{lof}_k(X)$ is larger than a threshold, then we classify $X$ is an outlier. In practice, the proportion of outliers is given by decision makers as an input and the threshold is computed by the algorithm. For more detailed description of the algorithm, readers could refer to \cite{lof}.

\section{Our Idea: Clustering-Based Modelling Methods}
\par
In this part, we will explicitly describe our algorithm on outlier classification. We focus on seperately modelling data distribution with different labels and apply a variant of Bayesian decision theory for classification. Such idea can be easily integrated with other supervised or unsupervised learning technique such as decision tree and EM algorithm.

\subsection{Bayesian Decision Theory}
\par
We assume the following conditions.
\begin{enumerate}
\item Suppose there are $k$ different classes $G_1, ..., G_k$. 
\item For any given data $x$, the prior probability that it belongs to $G_k$ is $p_k$ (Thus we must have $\sum_{i=1}^kp_i=1$). 
\item Let $L(j|i)$ be the loss when we classify an object in $G_i$ to $G_j$. Generally, when $i=j$, $L(j|i) = 0$.
\item Let $P(j|i;D)$ be the probability that we classify an object in $G_i$ into $G_j$ under the criterion $D$.
\end{enumerate}
\par
With these assumptions, we now obtain the following definition.

\begin{defn}
Let $g(D) := \sum_{i=1}^kp_i\sum_{j=1}^kP(j|i;D)L(j|i)$ to be the average loss under classification criterion $D$. If there exists a $D^*$ such that
\begin{eqt}
g(D^*) = \min_Dg(D)
\end{eqt}
We call $D^*$ the optimal Bayesian (classification) criterion.
\end{defn}
If we add some conditions, we will have the following theorem.
\begin{thm}
\label{optbayes}
Suppose the joint distribution density of the $i$th class $G_i$ is $f_i(X)$, then the optimal Bayesian criterion $D^*=(D_1^*, ..., D_k^*)$ is
\begin{eqt}
\label{optbayescrit}
D_i^* = \{X|h_i(X)<h_j(X), \forall j\neq i\},
\end{eqt}
where $h_j(X) = \sum_{i=1}^kp_iL(j|i)f_i(X)$, i.e., the average loss when we classify $X$ into $G_j$.
\end{thm}
\begin{proof}
For detailed proof, please see the appendix.
\end{proof}
\par
In our setting, $k=2$. Then from Theorem \ref{optbayes},
\begin{eqt}
h_1(X) = p_2f_2(X)L(1|2),\indent h_2(X) = p_1f_1(X)L(2|1)
\end{eqt}
Therefore, the optimal Bayesian solution is 
\begin{eqt}
&D_1 = \{X|W(X)>d\}, \\
&D_2 = \{X|W(X)\leqslant d\},
\end{eqt}
where
\begin{eqt}
\label{param}
W(X) = \frac{f_1(X)}{f_2(X)},\indent d = \frac{p_2L(1|2)}{p_1L(2|1)}.
\end{eqt}
Once we obtain the densities, the prior probabilities and the loss, we could make optimal Bayesian decision.

\subsection{GMDA: Gaussian Mixture Discriminant Analysis}
\par
To determine the density $f(X)$, we have numerous choices. For both fitness of generalization and speed of optimization, we choose to model $f(X)$ using Gaussian Mixture. Many kinds of data in real life can be well approximated by this model and well-performed algorithms such as EM algorithm are widely applied for optimization.
\par
For parameters in $d$(see \ref{param}), prior probabilities $p_1$ and $p_2$ could be inferred from data. However, generally, the loss $L(1|2)$ and $L(2|1)$ are determined by decision makers and we need to find a criterion to decide the ratio. Therefore, we should regard $d$ as a tuning parameter. 
\par
Based on previous analysis, our algorithm is as follows. We split the data into Training Set, Validation Set and Test Set. We first use Training Set to model the distribution of normal data ($f_1(X)$) and anomaly data ($f_2(X)$). We then use Validation Set to determine an optimal $d$. Finally, we apply the tuned $d$ to test performance on Test Set. The formal formulation is in Algorithm \ref{GMDA}.
\\\\
\begin{algorithm}[H]
\label{GMDA}
\caption{Gaussian Mixture Discriminant Analysis}
\SetAlgoLined
	\KwIn {\textit{T}: Training set\; \textit{V}: Validation set\; $n_1$: The number of clusters for normal data\; $n_2$: The number of clusters for anomaly data\; $D$: The list of possible choices of $d$\; $L$: An empty list to restore \textit{F-score}s.}
	\KwOut {$f_1(X)$: Density of Normal Data\; $f_2(X)$: Density of Anomaly Data\; $d$: Threshold.}
		Apply EM algorithm to normal data in \textit{T} to obtain a density function $f_1(X)$ with $n_1$ clusters\;
		Apply EM algorithm to anomaly data in \textit{T} to obtain a density function $f_2(X)$ with $n_2$ clusters\;
		\For {$d$ in $D$}{
			Classify data in \textit{V}. If $\log f_2(X) - \log f_1(X) > d$, classify $X$ as anomaly, else normal\;
			Compute \textit{F-score} and add it into \textit{L}\;
		}
		$\rho \gets \arg\max L$\; 
		$d\gets D[\rho]$\;
		\Return $f_1(X), f_2(X), d$.
\end{algorithm}
 
\subsection{Relation and Extension with Other Methods}

\subsubsection{Relation with Naive Bayes}
\par

Naive Bayes models data using Gaussian distribution, which is similar to GMDA. However, there is much difference between the two methods.
\begin{enumerate}
\item For convenience of optimization, Naive Bayes use one Gaussian distribution, while we use Gaussian Mixture Model.
\item In Naive Bayes, a crucial assumption is independence between variables. However, we do not impose this assumption. In fact, the goal of using Gaussian Mixture Model is to capture the correlation between variables.
\end{enumerate}

\subsubsection{Relation with LDA and QDA}
\par

Both LDA and QDA (see Section \ref{DA}) model data from different classes seperately. Also, they embed prior probabilities from Bayesian view. However, there is difference between LDA/QDA and GMDA.
\begin{enumerate}
\item In LDA/QDA, we model the data using one Gaussian distribution, while in GMDA we model with Gaussian Mixture.
\item LDA/QDA use distribution of different classes to estimate prior probabilities. While in GMDA, we also embed loss of misclassification and use validation sets to tune the threshold. 
\end{enumerate}

\subsubsection{Relation with Robust Covariance Estimator(RCE)}
\par

A big difference between RCE and GMDA is that RCE only models one class of data, i.e., the normal data. Enlightened by RCE, we can combine the two algorithms with each other. We can use normal data in the training set to find a Gaussian Mixture model, and then directly tuning the threshold on validation set without further modelling anomaly data. This is GMDA with normal data only, ``GMDA-n".

\subsubsection{Relation with Decision Tree}
\par

If we look at Algorithm \ref{GMDA} again, we could get further insight. First, $\log f_2(X) - \log f_1(X)$ can be thought as a projection from $\mathbb{R}^n$ to $\mathbb{R}$. Second, the validation process to choose optimal $d$ can be thought as finding a tree with one layer that maximize the \textit{F-score}. This is equivalent to maximize 
\begin{eqt}
2\bigg/\left(\frac{\text{TP}+\text{FP}}{\text{TP}}+\frac{\text{TP}+\text{FN}}{\text{TP}}\right) = \frac{2\text{TP}}{2\text{TP}+\text{FP}+\text{FN}},
\end{eqt}
or equivalently, minimize
\begin{eqt}
\frac{\text{FP} + \text{FN}}{\text{TP}}.
\end{eqt}
\par
In Decision Tree setting with two classes, this is, to some extent, similar to minimizing the misclassification rate. This enlighten us to combine Algorithm \ref{GMDA} with Decision Tree Classifier.

\begin{algorithm}[H]
\label{GMDA-DTC}
\caption{GMDA with Decision Tree Classifier}
\SetAlgoLined
	\KwIn {\textit{T}: Training set\; \textit{V}: Validation set\; $n_1$: The number of clusters for normal data\; $n_2$: The number of clusters for anomaly data\;}
	\KwOut {$f_1(X)$: Density of Normal Data\; $f_2(X)$: Density of Anomaly Data\; \textit{DTC}: A Decision Tree Classifier.}
		Apply EM algorithm to normal data in \textit{T} to obtain a density function $f_1(X) = \sum_{i=1}^{n_1}\pi_if_{1, i}(X)$\;
		Apply EM algorithm to anomaly data in \textit{T} to obtain a density function $f_2(X) = \sum_{j=1}^{n_2}\pi_if_{2, j}(X)$\;
		Construct new features based on $f_1(X)$, $f_2(X)$, $f_{1, i}(X)$, $f_{2, j}(X)$$(1\leqslant i\leqslant n_1, 1\leqslant j\leqslant n_2)$\;
		Use the new features above to build a Decision Tree Classifier\;
		Tune the parameters using \textit{V}\;
		\Return $f_1(X), f_2(X)$, \textit{DTC}.
\end{algorithm}


\section{Numerical Experiment}
\par In this section, we will discuss the implementation of previously described algorithms, and make comparison among them from numerical experiments. These numerical experiments are carried on a computer with 2 Intel Core i7-6500U CPUs and 16GiB RAM.

\subsection{Supervised Learning Methods}
\par For supervised learning methods, we have implemented several methods. The setting of these methods are described below
\begin{itemize}
	\item\textbf{Decision Tree(DTC): } We choose the criterion for splitting nodes and pruning the tree to be Gini index, and the maximum depth of the tree to be 6. Furthermore, we have done experiments with weighted tree, whose purpose is to balance the number of instances in each class. We have chosen two type of weights: $1:5$ and $1:10$.
	\item\textbf{LDA with bagging(LDA-Bag): }We choose the number of bagging LDA estimators to be 5.
	\item\textbf{QDA with bagging(QDA-Bag): }We choose the number of bagging QDA estimators to be 11.
	\item\textbf{Random Forest(RF): } We choose the criterion for splitting nodes and pruning the tree to be Gini index, and the number of estimators to be .
	\item\textbf{XGBoost(XGB): }We choose the maximum depth of each tree to be 4, regularization term $\lambda$ to be 0.5 and $\gamma$ to be 0.
	\item\textbf{Neural Network(NN): }We choose the structure of the neural network to be a 4-layer fully connected neural network, with 5, 4, 3, 3 neurons in each layers respectively.
\end{itemize}

\subsection{Outlier Detection Methods}
\par For outier detection methods, we have implemented three algorithms mentioned above. The parameters are as follows.
\begin{itemize}
\item \textbf{Robust Covariance Estimation(RCE): } We directly run the fitting process of the algorithm on the training set, which contains both normal and anomaly data. We then tune the threshold for $d$ (see \ref{mahadist}) using validation set. The optimal threshold we select is $1.01 \times 10^6$. 
\item \textbf{Local Outlier Factor(LOF): } We set the number of neighbors $k$ to be 50 and the proportion of outliers as 0.0008. During the training process, we only use the normal data in the training set because we observe that directly running the fitting process on the whole training set will worsen the performance.
\item \textbf{Isolation Forest(ILF): } We set the number of trees to be 250 and the proportion of outliers as 0.005. We only use the normal data in the training set, as what we have done in Local Outlier Factor. 
\end{itemize}
\subsection{GMDA and Related Methods}
\par We have also implemented several algorithms related to GMDA. We now explain them as follows.
\begin{itemize}
\item \textbf{Gaussian Mixture Discriminant Analysis(GMDA): } We model normal data with $n_1=3$ clusters and anomaly data with $n_2=3$ clusters in the training set. Note that all of the following methods are based on the two fitted gaussian mixture (normal and anomaly). 
\item \textbf{Gaussian Mixture Discriminant Analysis with notmal data only(GMDA-n): } We use the fitted distribution of normal data obtained by GMDA. 
\item \textbf{Gaussian Mixture with Decision Tree Classifier(GM-DTC): } The algorithm is consistent with \ref{GMDA-DTC}. The features we select are
\begin{eqt}
\label{feature}
f_1(X) - f_2(X), \ f_{1, i}(X), \ f_{2, j}(X)\ (1\leqslant i\leqslant n_1, 1\leqslant j\leqslant n_2).
\end{eqt}
We choose Entropy Loss as the criterion. To prevent overfitting, we restrict the maximal depth of the tree to 5. 
\item \textbf{Gaussian Mixture with Bagging(GM-bag): } This algorithm is based on GM-DTC. We choose 11 tree estimators with the same features in (\ref{feature}). 
\item \textbf{Gaussian Mixture with Voting(GM-vote): } This algorithm is based on GM-DTC. To increase diversity, we choose 9 different tree estimators with the same features in (\ref{feature}), but with different weights(see Section \ref{weightdtc}), which are
\begin{eqt}
100:1,\ 50:1,\ 20:1,\ 10:1,\ 1:1,\ 1:10,\ 1:20,\ 1:50,\ 1:100.
\end{eqt}
\item \textbf{Gaussian Mixture with XGBoost(GM-xgb): } This algorithm is based on GM-DTC. We choose the maximal depth of each tree to be 5, regularization term $\lambda$ to be 0.5, and $\gamma$ to be 0.

\end{itemize}

\subsection{Comparison between Different Methods}
\par We have implemented several previous described algorithms together with algorithms proposed by ourselves, and the results can be viewed in Table \ref{comparison}.

\begin{table}[h]
	\centering
	\setlength{\belowcaptionskip}{10pt}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
	\hline
	Methods & \multicolumn{3}{|c|}{Train} & \multicolumn{3}{|c|}{Valid} & \multicolumn{3}{|c|}{Test} & \multicolumn{2}{|c|}{Time}\\
	\hline
	& R & P & F & R & P & F & R & P & F & T & P\\
	\hline
Logistic & 58.6 & 85.2 & 64.9 & 61.2 & 88.2 & 67.6 & 59.6 & 86.8 & 66.0 & 8.88 & 0.01\\
\hline
NB & 82.4 & 5.9 & 16.5 & 84.7 & 6.1 & 17.1 & 82.8 & 5.8 & 16.2 & 0.09 & 0.02\\
\hline
DTC & 83.4 & 98.4 & 87.5 & 81.6 & 90.9 & 84.3 & 79.8 & 91.9 & 83.2 & 4.02 & 0.01\\
\hline
DTC 1:5 & 84.7 & 98.0 & 88.4 & 79.6 & 91.8 & 83.0 & 76.8 & 92.7 & 81.1 & 4.19 & 0.01\\
\hline
DTC 1:10 & 85.8 & 94.1 & 88.2 & 83.7 & 87.2 & 84.7 & 79.8 & 86.8 & 81.8 & 4.16 & 0.01\\
\hline
LDA & 74.2 & 85.5 & 77.4 & 82.7 & 88.0 & 84.2 & 77.8 & 87.5 & 80.5 & 0.70 & 0.01\\
\hline
LDA-Bag & 74.9 & 85.7 & 77.9 & 82.7 & 88.0 & 84.2 & 77.8 & 87.5 & 80.5 & 4.20 & 0.02\\
\hline
QDA & 87.5 & 5.4 & 15.4 & \textbf{90.8} & 5.6 & 16.1 & \textbf{84.8} & 5.3 & 15.1 & 0.34 & 0.03\\
\hline
QDA-Bag & 87.1 & 5.7 & 16.2 & \textbf{90.8} & 6.0 & 17.1 & \textbf{84.8} & 5.7 & 16.0 & 5.30 & 0.27\\
\hline
RF & \textbf{96.3} & \textbf{100.0} & \textbf{97.4} & 81.6 & 93.0 & 84.8 & 74.7 & 93.7 & 79.7 & 11.78 & 0.06\\
\hline
NN & 78.3 & 84.9 & 80.2 & 82.7 & 84.4 & 83.2 & 78.8 & 82.1 & 79.8 & 7.54 & 0.01\\
\hline
XGBoost & 87.5 & 99.2 & 90.8 & 82.7 & \textbf{94.2} & 85.9 & 80.8 & \textbf{94.1} & \textbf{84.5} & 37.29 & 0.13\\
\hline
GMDA & 79.0 & 80.3 & 79.4 & 85.7 & 81.6 & 84.4 & 80.8 & 82.5 & 81.3 & 55.55 & 0.11 \\
\hline
GMDA-n & 77.6 & 82.1 & 78.9 & 84.7 & 83.0 & 84.2 & 79.8 & 84.9 & 81.3 & 6.69 & 0.06 \\
\hline
GM-dtc & 82.7 & 92.4 & 85.5 & 83.7 & 85.4 & 84.2 & 78.8 & 89.7 & 81.8 & 1.99 & 0.20 \\
\hline
GM-bag & 83.4 & 94.6 & 86.5 & 84.7 & 90.2 & 86.3 & 79.8 & 92.9 & 83.4 & 8.23 & 0.25 \\
\hline
GM-vote & 82.0 & 95.3 & 85.7 & 84.7 & 88.3 & 85.8 & 78.8 & 94.0 & 82.9 & 7.95 & 0.25 \\
\hline
GM-xgb & 86.8 & 98.8 & 90.2 & 84.7 & 91.2 & \textbf{86.6} & 80.8 & \textbf{94.1} & \textbf{84.5} & 13.88 & 0.33 \\
\hline
RCE & 74.9 & 3.5 & 10.4 & 79.6 & 3.9 & 11.5 & 80.8 & 3.8 & 11.3 & 20.19 & 0.05 \\
\hline
LOF & 50.8 & 54.5 & 51.9 & 55.1 & 53.5 & 54.6 & 50.5 & 51.5 & 50.8 & 368.53 & 122.77 \\
\hline
ILF & 47.5 & 14.1 & 27.5 & 46.9 & 13.6 & 26.8 & 47.5 & 14.3 & 27.7 & 368.53 & 3.96 \\
\hline
	\end{tabular}
	\caption{Comparison between different methods. Each row represent one kind of algorithm. The first eleven rows are about supervised learning method, and the next six rows are about Gaussian mixture model methods, and the last three rows are about outlier detection methods. We have presented the recall (R), precision (P) and F-score (F) on the training, validation and test sets above, where the definition of recall, precision and F-score are given in the first section. The last two columns are about time costs on training process and prediction process. (on test set)}
	\label{comparison}
\end{table}

\section{Conclusion}


\bibliography{reference}
\bibliographystyle{plain}

\newpage
\begin{appendix}
\section{Proof of Theorem \ref{optbayes}}
\par

We have
\begin{eqt}
g(D^*) & = \sum_{i=1}^kp_i\sum_{j=1}^kP(j|i;D^*)L(j|i) \\
& = \sum_{i=1}^kp_i\sum_{j=1}^kL(j|i)\int_{D_j^*}f_i(X)dX \\
& = \sum_{j=1}^k\int_{D_j^*}\sum_{i=1}^kp_iL(j|i)f_i(X)dX \\
& = \sum_{j=1}^k\int_{D_j^*}h_j(X)dX.
\end{eqt}
For any partition of $\mathbb{R}^n$, $D = \{D_1, ..., D_k\}$, we also have
\begin{eqt}
g(D) = \sum_{j=1}^k\int_{D_j}h_j(X)dX.
\end{eqt}

Therefore, 
\begin{eqt}
g(D^*) - g(D) & = \sum_{j=1}^k\int_{D_j^*}h_j(X)dX - \sum_{t=1}^k\int_{D_t}h_t(X)dX \\
& = \sum_{j=1}^k\sum_{t=1}^k\int_{D_j^*\cap D_t}\left(h_j(X)-h_t(X)\right)dX \\
& \leqslant 0.
\end{eqt}
In fact, $\forall j, t\in[k]$, we have $h_j(X)\leqslant h_t(X)$ on $D_j^*\cap D_t$ from the definition of $D^*$. This completes the proof.
\end{appendix}

\end{document}
